\input{../../settings}

\title{Теория вероятности и математическая статистика, 2 курс, 2 семестр}
\begin{document}

\author{@defunator}
\cfoot{\thepage\ of \pageref{LastPage}}
\maketitle


\tableofcontents

\clearpage

\section{Сходимости случайных величин}

\begin{definition}
Последовательность случайных величин $\xi_{n}$ сходится к случайной величине $\xi$:
\begin{enumerate}
\item \textit{Почти наверное}($\xi_{n} \xrightarrow{\text{п.н.}} \xi$), если 
\[
    P\left(\lim_{n \to \infty} \xi_{n} = \xi\right) = 1
\]
\item \textit{По вероятности}($\xi_{n} \xrightarrow{\text{p}} \xi$), если
\[
    \forall \varepsilon > 0 \ \lim_{n \to \infty} P\left(|\xi_{n} - \xi| \geq \varepsilon\right) = 0
\]
\item \textit{По распределению}($\xi_{n} \xrightarrow{\text{d}} \xi$), если 
\[
    \lim_{n \to \infty} F_{\xi_{n}}\left(x\right) = F_{\xi}\left(x\right)
\]
для любых $x$, в которых непрерывна $F_{\xi}$
\end{enumerate}
\end{definition}

\begin{theorem}(Эквивалетное определение сходимости по распределению)
$\xi_{n} \xrightarrow{d} \xi$  $\Leftrightarrow$ $\forall g \ -$ непрерывна и ограничена на $\mathbb{R}$ верно
$\lim_{n \to \infty}  \mathbb{E}  g\left(\xi_{n}\right) =  \mathbb{E}  g\left(\xi\right)$

\end{theorem}
\begin{proof}
$\Rightarrow$ \\

Пусть $t \ -$ точка непрерывности $F_{\xi}\left(t\right)$. Заметим, что $F_{\xi}\left(t\right) = P\left(\xi \in (-\infty, t]\right)=$\\$ = \mathbb{E}  \text{Ind}_{(-\infty, t]}\left(\xi\right)$. \\
В силу:\\
(1) $\mathbb{E} \text{Ind}_{(a_i, b_i]}(\xi_n) = F_{\xi_n}(b_i) - F_{\xi_n}(a_i) \xrightarrow{n \xrightarrow{} \infty} F_{\xi}(b_i) - F_{\xi}(a_i) =\mathbb{E} \text{Ind}_{(a_i, b_i]}(\xi)$\\
(2) Линейность предела (с какими-то коэффициентами $c_i$)\\
Верна следующая сходимость:
\[
    \mathbb{E}\sum\limits_{i = 1}^{N} c_i \cdot \text{Ind}_{(a_i, b_i]}(\xi_n) \xrightarrow{} \mathbb{E}\sum\limits_{i = 1}^{N} c_i \cdot \text{Ind}_{(a_i, b_i]}(\xi)
\]
Теперь нам бы хотелось от непрерывной ограниченной функции на прямой перейти к функции на отрезке, а там мы уже сможем ее приблизить ступенчатой и воспользоваться предыдудщим утверждением и все доказать. Мы знаем, что \\ $\forall \varepsilon > 0 \ \exists A$: $P\left(-A < \xi \leq A\right) > 1 - \varepsilon$ (потому что $P\left(\xi \in \mathbb{R}\right) = 1$). Тогда получаем: 
$$F_{\xi_{n}}\left(A\right) - F_{\xi_{n}}\left(-A\right) \xrightarrow{n \to \infty} F_{\xi}\left(A\right) - F_{\xi}\left(-A\right) = P\left(-A < \xi \leq A\right) > 1 - \varepsilon$$ То есть для $\forall \varepsilon > 0 \ \exists N \ \forall n > N$ верно:
$$\abs{\left(F_{\xi_{n}}\left(A\right) - F_{\xi_{n}}\left(-A\right)\right) - \left(F_{\xi}\left(A\right) - F_{\xi}\left(-A\right)\right)} < \varepsilon$$ 
Комбинируя последние два утверждения, получаем для $\forall \varepsilon > 0\ \exists A \ \exists N \ \forall n > N$: $$F_{\xi_n}\left(A\right) - F_{\xi_n}\left(-A\right) > F_{\xi}\left(A\right) - F_{\xi}\left(-A\right) - \varepsilon > 1 - 2 \varepsilon$$ 
Из чего следует $\forall \varepsilon > 0  \ \exists A \ \exists N\ \forall n > N$: 
\[
    P\left(-A \leq \xi_{n} \leq A\right) \geq F_{\xi_{n}}\left(A\right) - F_{\xi_{n}}\left(-A\right) > 1 - \varepsilon
\]
Теперь возьмем любую непрерывную ограниченную функцию $g$, приблизим ее на отрезке $\left[-A, A\right]$ ступенчатой функцией $g_{\varepsilon}$, что $\abs{g\left(x\right) - g_{\varepsilon}\left(x\right)} < \varepsilon$, а вне отрезка положим $g_{\varepsilon} = 0$. Имеем $\forall \varepsilon > 0 \ \exists A \ \forall n$: 
\[
\abs{ \mathbb{E}  g\left(\xi_{n}\right) -  \mathbb{E}  g\left(\xi\right)} \leq \abs{ \mathbb{E}  \left(1 - \text{Ind}_{\left[-A, A\right]}\left(\xi_{n}\right)\right) \cdot g\left(\xi_{n}\right) -  \mathbb{E}  \left(1 - \text{Ind}_{\left[-A, A\right]}\left(\xi\right)\right)g\left(\xi\right)} + 
\]
\[
    + \abs{ \mathbb{E}  \text{Ind}_{\left[-A, A\right]}\left(\xi_{n}\right) \cdot g\left(\xi_{n}\right) -  \mathbb{E}  \text{Ind}_{\left[-A, A\right]}\left(\xi\right) \cdot g\left(\xi\right)}
\]
Ясно, что первый модуль $< C \cdot 2\varepsilon$ (из ограниченности $g$ $\forall x_1, \ x_2$: $\abs{g\left(x_1\right) - g\left(x_2\right)} < C$ и так как $P\left(\abs{\xi} > A\right), \ P\left(\abs{\xi_{n}} > A\right) < \varepsilon$). А во втором модуле $g$ заменим на $g_{\varepsilon}$ с погрешностью $\varepsilon$, то есть он 
$< 2\varepsilon + \abs{ \mathbb{E}  \text{Ind}_{\left[-A, A\right]}\left(\xi_{n}\right) \cdot g_{\varepsilon}\left(\xi_{n}\right) -  \mathbb{E}  \text{Ind}_{\left[-A, A\right]}\left(\xi\right) \cdot g_{\varepsilon}\left(\xi\right)}$. А про оставшийся модуль мы уже знаем, что он сходится к 0, так как ступенчатая функция(то есть $< \varepsilon$ для $\forall n > N$). В итоге имеем:
$\abs{ \mathbb{E}  g\left(\xi_{n}\right) -  \mathbb{E}  g\left(\xi\right)} < \varepsilon$.
То есть $\lim_{n \to \infty}  \mathbb{E}  g\left(\xi_{n}\right) =  \mathbb{E}  g\left(\xi\right)$ \\
$\Leftarrow$ \\
Пусть $t \ -$ точка непрерывности $F_{\xi}\left(t\right)$. Мы знаем, что $F_{\xi}\left(t\right) =  \mathbb{E}  \text{Ind}_{(-\infty, t]}\left(\xi\right)$.
Для $\forall \delta > 0$ определим функции: 
\[
    g_{- \delta}\left(x\right) = 
    \begin{cases}
        1 & x < t - \delta \\
        \frac{1}{\delta} \cdot \left(t - x\right) & t - \delta \leq x \leq t \\
        0 & t < x
    \end{cases}
\]
\[
    g_{+ \delta}\left(x\right) = 
    \begin{cases}
        1 & x < t \\
        \frac{1}{\delta} \cdot \left(t - x\right) & t \leq x \leq t + \delta \\
        0 & t + \delta < x
    \end{cases}
\]
Заметим, что $\forall x$: $$
\text{Ind}_{(- \infty, t - \delta]}\left(x\right) \leq g_{-\delta}\left(x\right) \leq \text{Ind}_{(-\infty, t]}\left(x\right) \leq g_{+\delta}\left(x\right) \leq \text{Ind}_{(\infty, t + \delta]}\left(x\right)$$
Взяв матожидания ($x = \xi_{n}$) от второго и третьего неравенств, получим: $$ \mathbb{E}  g_{-\delta}\left(\xi_{n}\right) \leq F_{\xi_{n}}\left(t\right) \leq  \mathbb{E}  g_{+ \delta}\left(\xi_{n}\right)$$
Теперь устремим $n \to \infty$: 
\[
     \mathbb{E}  g_{-\delta}\left(\xi\right) \leq \inf \lim_{n \to \infty} F_{\xi_{n}}\left(x\right) \leq \sup \lim_{n \to \infty} F_{\xi_{n}}\left(x\right) \leq  \mathbb{E}  g_{+ \delta}\left(\xi \right)
\]
Рассмотрим первое и последнее неравенство той цепочки($x = \xi$ и возьмем то него матожидание), получим: 
\[
    F_{\xi}\left(t - \delta\right) \leq  \mathbb{E}  g_{- \delta}\left(\xi\right) \leq  \mathbb{E}  g_{+ \delta}\left(\xi\right) \leq F_{\xi}\left(t + \delta\right)
\]
Теперь $\delta \to 0$: 
\[
    F_{\xi}\left(t\right) =  \mathbb{E}  g_{- \delta}\left(\xi\right) = inf \lim_{n \to \infty} F_{\xi_{n}}\left(t\right) = sup \lim_{n \to \infty} F_{\xi_{n}}\left(t\right) =  \mathbb{E}  g_{+ \delta}\left(\xi\right)
\]
Получаем: $F_{\xi}\left(t\right) = \lim_{n \to \infty} F_{\xi_{n}}\left(t\right)$

\end{proof}

\begin{theorem}
$
    \xi_{n} \xrightarrow{\text{п.н.}} \xi \Rightarrow \xi_{n} \xrightarrow{\text{p}} \xi
$
\end{theorem}
\begin{proof}
Знаем $P\left(\lim_{n \to \infty} \xi_{n} = \xi\right) =1 $. Заметим вложенность следующих событий для $\forall \varepsilon > 0$: $\left\{\lim_{n \to \infty} \xi_{n} = \xi\right\} \Rightarrow \bigcup_{N = 1}^{\infty} \bigcap_{n = N}^{\infty} \left\{\abs{\xi_{n} - \xi} < \varepsilon\right\}$ (это по сути и есть определение предела, что для $\forall \varepsilon$, начиная с некоторого $N$ выполнятется условие). То есть: 
$$1 = P\left(\lim_{n \to \infty} \xi_{n} = \xi\right) \leq P\left(\bigcup_{N = 1}^{\infty} \bigcap_{n = N}^{\infty} \left\{\abs{\xi_{n} - \xi} < \varepsilon\right\}\right) = 1$$ 
Так как последовательность множеств $A_{N} = \bigcap_{n = N}^{\infty} \left\{\abs{\xi_{n} - \xi} < \varepsilon\right\}$ расширяющаяся ($A_{N} \subseteq A_{N + 1}$), в объединении они дают событие вероятности 1, значит по теореме непрерывности $\lim_{N \to \infty} P\left(A_{N}\right) = 1$. 
Теперь заметим: $P\left(A_{N}\right) \leq P\left(\abs{\xi_{N} - \xi} < \varepsilon\right)$.
То есть: $\lim_{N \to \infty} P\left(\abs{\xi_{N} - \xi} < \varepsilon\right) = 1$.
Или что то же самое: $\lim_{N \to \infty} \left(1-P\left(\abs{\xi_{N} - \xi} < \varepsilon\right)\right) = \lim_{N \to \infty} P\left(\abs{\xi_{N} - \xi} \geq \varepsilon\right) = 0 \ -$ определение сходимости по вероятности.
\end{proof}
\begin{theorem}(Теорема Лебега о мажорируемой сходимости) 
$\xi_{n} \xrightarrow{\text{p}} \xi$ и $\abs{\xi_{n}}, \abs{\xi} \leq \eta$ п. н. (где $\eta \ -$ случайная величина, что $ \mathbb{E}  \eta < \infty$), то $ \mathbb{E}  \xi_{n} \to  \mathbb{E}  \xi$
\end{theorem}
\begin{proof}
Докажем теорему в частном случае, когда $\eta \equiv C$.
$\forall \varepsilon > 0 \forall n $ $\abs{ \mathbb{E}  \xi_{n} -  \mathbb{E}  \xi} \leq  \mathbb{E}  \abs{\xi_{n} - \xi} =  \mathbb{E}  \abs{\xi_{n} - \xi} \cdot \text{Ind}_{\abs{\xi_{n} - \xi} \geq \varepsilon} +  \mathbb{E}  \abs{\xi_{n} - \xi} \cdot \text{Ind}_{\abs{\xi_{n} - \xi} < \varepsilon} \leq 2C \cdot P\left(\abs{\xi_{n} - \xi} \geq \varepsilon\right) + \varepsilon \cdot 1$.
Так как $\xi_{n} \xrightarrow{\text{p}} \xi$, то $\exists N \forall n > N$: $P\left(\abs{\xi_n - \xi} \geq \varepsilon\right) < \varepsilon$.
Тогда получаем: $\abs{ \mathbb{E}  \xi_{n} -  \mathbb{E}  \xi} < 2C \cdot \varepsilon + \varepsilon$. То есть $ \mathbb{E}  \xi_n \to  \mathbb{E}  \xi$.

\end{proof}

\begin{advice}
$\xi_{n} \xrightarrow{\text{p}} \xi $ $\Rightarrow$ для $\forall g \ -$ непрерывная, $g\left(\xi_{n}\right) \xrightarrow{\text{p}} g\left(\xi\right)$
\end{advice}
\begin{proof}
Знаем для любой случайной величины $\forall \varepsilon > 0 \exists A$: $P\left(\abs{\xi} > \frac{A}{2}\right) < \varepsilon$.
$\exists N \forall n > N$: 
$$P\left(\abs{\xi_{n}} > A\right) \leq P\left(\abs{\xi - \xi_{n}} + \abs{\xi} > A\right) \leq P\left(\abs{\xi - \xi_{n}} > \frac{A}{2} \vee \abs{\xi} > \frac{A}{2}\right) \leq P\left(\abs{\xi - \xi_{n}} > \frac{A}{2}\right) + P\left(\abs{\xi} > \frac{A}{2}\right) < \varepsilon$$.
Теперь возьмем $g$, она равномерно непрерывна на $\left[-A, A\right]$:
\[
    \forall \varepsilon > 0 \exists \delta > 0: \ \abs{x - y} < \delta \Rightarrow \abs{g\left(x\right) - g\left(y\right)} < \varepsilon \ \forall x, y \in \left[-A, A\right]
\]
Докажем $g\left(\xi_{n}\right) \xrightarrow{\text{p}} g\left(\xi\right)$:
\[
    P\left(\abs{g\left(\xi_n\right) - g\left(\xi\right)} \geq \varepsilon\right) = P\left(\abs{g\left(\xi_n\right) - g\left(\xi\right)} \geq \varepsilon \ |\  \xi_n, \xi \in \left[-A, A\right]\right) \cdot P\left(\xi_n, \xi \in \left[-A, A\right]\right) +
    \]
    \[+ P\left(\abs{g\left(\xi_n\right) - g\left(\xi\right)} \geq \varepsilon \ |\  \xi_n, \xi \not \in \left[-A, A\right]\right) \cdot P\left(\xi_n, \xi \not \in \left[-A, A\right]\right)
\]
Посмотрев на определение равномерное непрерывности, заметим, что: 
\[
P\left(\abs{g\left(\xi_n\right) - g\left(\xi\right)} \geq \varepsilon \ |\  \xi_n, \xi \in \left[-A, A\right]\right) \leq P\left(\abs{\xi_n - \xi} \geq \delta\right)
\]
 А это уже, так как у нас есть сходимость по вероятности $\to 0$.
 И заметим, что 
 \[
    P\left(\xi_n, \xi \not \in \left[-A, A\right]\right) \leq P\left(\abs{\xi_n} > A\right) + P\left(\abs{\xi} > A\right) < 2 \varepsilon
 \]
 То есть $\to 0$ при $n, A \to \infty$
Все, получили, что $P\left(\abs{g\left(\xi_n\right) - g\left(\xi\right)} \geq \varepsilon\right) \to 0$ при  \\ $n, A \to \infty$.
\end{proof}

\begin{corollary}
$\xi_{n} \xrightarrow{\text{p}} \xi \Rightarrow \xi_{n} \xrightarrow{\text{d}} \xi$
\end{corollary}
\begin{proof}
Берем предыдущее предложение. Потом используем теорему Лебега для произвольной непрерывной ограниченной $g$ и вспоминаем эквивалентное опеределение сходимости по распределению.
\end{proof}

\begin{theorem} (Эквивалетное опеределение сходимости почти наверное) \\
$$\xi_n \xrightarrow{\text{п.н.}} \xi \Leftrightarrow \forall \varepsilon > 0 : \ \lim_{n \to \infty} P\left(\sup_{k \geq n} \abs{\xi_k - \xi} > \varepsilon\right) = 0$$
\end{theorem}
\begin{proof}
Рассмотрим следующие события:
$$A_{k}^{\varepsilon} = \left\{\abs{\xi_k - \xi} > \varepsilon \right \}$$
$$A^{\varepsilon} = \bigcap_{n = 1}^{\infty} \bigcup_{k \geq n} A_{k}^{\varepsilon}$$
Заметим, что:
$$\left\{\sup_{k \geq n} \abs{\xi_{k} - \xi} > \varepsilon\right\} = \left\{\exists k \geq n: \ \abs{\xi_k - \xi} >\varepsilon\right\} = \bigcup_{k \geq n} A_{k}^{\varepsilon}$$
$$\left\{\lim_{n \to \infty} \xi_{n} \neq \xi\right\} = \left\{\exists \varepsilon > 0 \forall n \exists k \geq n : \ \abs{\xi_{k} - \xi} \geq \varepsilon\right\} = \bigcup_{m = 1}^{\infty} \bigcap_{n}^{\infty} \bigcup_{k \geq n} A_{k}^{\varepsilon = \frac1m} = \bigcup_{m = 1}^{\infty} A^{\frac1m}$$
Тогда имеем:
$$\xi_n \xrightarrow{\text{п.н.}} \xi \Leftrightarrow P\left(\lim_{n \to \infty} \xi_{n} \neq \xi\right) = 0 \Leftrightarrow
P\left(\bigcup_{m = 1}^{\infty} A^{\frac1m}\right) = 0 \Leftrightarrow \forall m \ P\left(A^{\frac1m}\right) = 0 \Leftrightarrow
\forall \varepsilon > 0 \ P\left(A^{\varepsilon}\right) = 0 \Leftrightarrow$$
Теперь заметим вложенность последовательности событий $B_{n}^{\varepsilon} = \bigcup_{k \geq n} A_{k}^{\varepsilon}$ и, взглянув на определение $A^{\varepsilon}$, по теореме о непрерывности вероятностной меры продолжаем цепочку:
$$\Leftrightarrow \lim_{n \to \infty} P\left(\bigcup_{k \geq n} A_{k}^{\varepsilon}\right) = 0 \Leftrightarrow
\lim_{n \to \infty} P\left(\sup_{k \geq n} \abs{\xi_{k} - \xi} > \varepsilon\right) = 0$$
\end{proof}

Теперь приведем некоторые примеры, опровергающие остальные следствия сходимостей
\begin{example}
$$\xi_{n} \xrightarrow{\text{d}} \xi \not\Rightarrow \xi_n \xrightarrow{\text{P}} \xi$$
Пусть $\Omega = \left\{\omega_{1}, \omega_2\right\}$, $P\left(\left\{\omega_i\right\}\right) = \frac12$. Определим $\forall n$ $\xi_n\left(\omega_i\right) = \left(-1\right) ^ i$. Положим $\xi = -\xi_n$. Тогда $\forall f$ непрерыной и ограниченной:
$$ \mathbb{E}  f\left(\xi_n\right) = \frac{f\left(1\right) + f\left(-1\right)}{2} =  \mathbb{E}  f\left(\xi\right)$$
То есть $\xi_{n} \xrightarrow{\text{d}} \xi$. Однако $\forall n \ \abs{\xi_n - \xi} = 2$, то есть $\xi_n \not\xrightarrow{\text{P}} \xi$
\end{example}
\begin{example}
$$\xi_{n} \xrightarrow{\text{P}} \xi \not\Rightarrow \xi_n \xrightarrow{\text{п.н.}} \xi$$
Возьмем $\Omega = \left[0, 1\right]$, $\xi_{2 ^ n + p} = \text{Ind}_{\left[\frac{p}{2 ^ n}, \frac{p + 1}{2 ^ n}\right]}$, $0 \leq p < 2^ n$. Ясно, что $\xi_{n} \xrightarrow{\text{P}} \xi$, так как 
$$\lim_{n \to \infty} P\left(\xi_{n} > 0\right) = \lim_{n \to \infty} \frac{1}{2 ^ {\lfloor \log n \rfloor}} = 0$$, но 
$\xi_n \not\xrightarrow{\text{п.н.}} 0$, так как для любого исхода $\omega$ существует бесконечно много $n$, что $\xi_{n}\left(\omega\right) = 1$. Теперь осталось посмотреть на теорему 4 и все станет ясно.
\end{example}
\begin{example}
$$\xi_{n} \xrightarrow{\text{d}} \xi \not\Rightarrow \xi_n \xrightarrow{\text{п.н.}} \xi$$
Если бы следствие имело место, то отсюда вытекало бы, что:
$$\xi_{n} \xrightarrow{\text{d}} \xi \not\Rightarrow \xi_n \xrightarrow{\text{P}} \xi$$
противоречие

\end{example}
\clearpage





\section{Характеристические функции}

\begin{definition}
\textit{Характеристической функцией} случайной величины $\xi$ называется функция:
$$\varphi_{\xi}\left(t\right) =  \mathbb{E}  e ^ {it \xi} =  \mathbb{E}  \left(\cos\left(t \xi\right) + i \sin\left(t \xi\right)\right)$$
\end{definition}

\begin{advice}
(Свойства характеристических функций)
\begin{enumerate}
\item $\varphi_\xi\left(0\right) = 1$, $\abs{\varphi_\xi\left(t\right)} \leq 1$ $\forall t \in \mathbb{R}$
\item $\varphi_{a\xi+b}\left(t\right) = e ^ {itb} \varphi_\xi\left(at\right)$
\item если $\xi_1, \cdots \xi_n \ - $ независимые случайные величины и $S = \xi_1 + \cdots + \xi_n$, то 
\[
    \varphi_S\left(t\right) = \varphi_{\xi_1}\left(t\right) \cdots \varphi_{\xi_n}\left(t\right)
\]
\end{enumerate}
\end{advice}
\begin{proof}
.
\begin{enumerate}
\item Понятно, так как в матожидании могут быть только комплексные числа с модулем $\leq 1$.
\item $\varphi_{a\xi+b}\left(t\right) =  \mathbb{E}  e ^ {it \left(a\xi + b\right)} = e ^ {itb}  \mathbb{E}  e ^ {i \left(at\right) \xi} = e ^ {itb} \varphi_{\xi}\left(at\right)$
\item $\varphi_{S}\left(t\right) =  \mathbb{E}  \left(e ^ {it \xi_1} \cdots e ^ {it \xi_n}\right) =  \mathbb{E}  e ^ {it\xi_1} \cdots  \mathbb{E}  e ^ {it \xi_n} = \varphi_{\xi_1}\left(t\right) \cdots \varphi_{\xi_n}\left(t\right)$
\end{enumerate}
\end{proof}

\begin{example}
Вычислим $\varphi_{\xi}\left(t\right)$, где $\xi \sim N\left(0, 1\right)$:
\[
    \varphi_{\xi}\left(t\right) =  \mathbb{E}  e ^ {it\xi} = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \cos\left(tx\right) e ^ {-\frac{x ^2}{2}} dx + \frac{i}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \sin\left(tx\right) e ^ {-\frac{x ^2}{2}} dx
\]
Заметим, что $\int_{-\infty}^{+\infty} \sin\left(tx\right) e ^ {-\frac{x ^2}{2}} dx = 0$, так как это интеграл от нечетной функции по симметричному промежутку. Тогда имеем:
\[
    \varphi_{\xi}\left(t\right) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \cos\left(tx\right) e ^ {-\frac{x ^2}{2}} dx
\]
Возьмем производную по $t$:
\[
    \varphi_{\xi}'\left(t\right) = -\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} x\sin\left(tx\right) e ^ {-\frac{x ^ 2}{2}} dx
     = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \sin\left(tx\right) d\left(e ^ {-\frac{x ^ 2}{2}}\right)=\]
\[
    = \frac{1}{\sqrt{2 \pi}} \sin\left(tx\right) e ^ {-\frac{x ^ 2}{2}} |_{-\infty}^{+\infty} - \frac{t}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \cos\left(tx\right) e ^ {-\frac{x ^ 2}{2}} dx = 0 -\frac{t}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \cos\left(tx\right) e ^ {-\frac{x ^ 2}{2}} dx = - t \varphi_{\xi}\left(t\right)
\]
Теперь надо решить дифференциальное уравнение: 
$$ \varphi_\xi'\left(t\right) = - t \varphi_{\xi}\left(t\right)$$
$$\frac{\varphi_\xi'\left(t\right)}{\varphi_\xi\left(t\right)} = -t$$
Интегрируем обе части:
$$ \int \frac{ d\left(\varphi_\xi\left(t\right)\right)}{\varphi_\xi\left(t\right)} = \ln \abs{\varphi_\xi\left(t\right)} + C = \int -t dt = -\frac{t ^ 2}{2}$$
$$\varphi_\xi\left(t\right) = C' e ^ {-\frac{t ^ 2}{2}}$$
$$\varphi_\xi\left(0\right) = 1 = C' \Rightarrow C' = 1$$
(Ответ никак нулевым быть не может, поэтому, когда мы поделили на хар функцию ничего плохого не произошло)
\end{example}
\begin{advice}
Пусть случайная величина $\xi$ обладает конечным $k$-тым моментом, то есть $ \mathbb{E}  \abs{\xi} ^ k < \infty$. Тогда $\varphi$ имеет непрерывную $k$-тую производную и
$
    \varphi^{(k)}\left(0\right) = i ^ k  \mathbb{E}  \xi ^ k
$
\end{advice}
\begin{proof}
Заметим, что:
\[
    \abs{\frac{e ^ {i \Delta t \xi} - 1}{\Delta t}} \leq \frac{\sqrt{\left(\cos\left(\Delta t\xi\right) - 1\right) ^ 2 + \sin ^ 2\left(\Delta t\xi\right)}}{\abs{\Delta t}} = \frac{\sqrt{2 - 2 \cos\left(\Delta t\xi\right)}}{\abs{\Delta t}} = \frac{2 \abs{\sin\left(\frac{\Delta t\xi}{2}\right)}}{\Delta t} \leq \frac{2 \cdot \frac{\abs{\Delta t\xi}}{2}}{\abs{\Delta t}} = \abs{\xi}
\]
Возьмем вместо $\Delta t$ последовательность $a_n \to 0$, Получим, что последовательность случайных величин, сходящихся почти наверное и ее предел ($i \xi$) ограничены случайной величиной ($\xi$)с конечным ожиданием, получаем по теорема Лебега: \[
    \mathbb{E}  \frac{e ^ {i a_n\xi} - 1}{a_n} \to  \mathbb{E}  i \xi
\]
Теперь посчитаем производную хар фукнции:
\[
    \varphi'\left(t\right) = \lim_{\Delta t \to 0} \frac{\varphi\left(t + \Delta t\right) - \varphi\left(t\right)}{\Delta t} = \lim_{\Delta t\to 0}  \mathbb{E}  e ^ {i t \xi} \cdot \frac{e ^ {i \Delta t \xi} - 1}{\Delta t} = i  \mathbb{E}  e ^ {it \xi} \xi
\]
Теперь очевидна непрерывность первой производной и $\varphi'\left(0\right) = i  \mathbb{E}  \xi$, для производных высших порядков аналогично

\end{proof}


\begin{theorem}
\[
    \xi_n \xrightarrow{\textit{d}} \xi \Leftrightarrow \forall t \lim_{n \to \infty} \varphi_{\xi_n}\left(t\right) = \varphi_{\xi}\left(t\right)
\]
\end{theorem}

\begin{proof}
$\Rightarrow$ \\
Очевидно по Теореме 1 \\
$\Leftarrow$ \\
Докажем при условиии $\sup_{n}  \mathbb{E}  \xi_n^2 \leq C < \infty$. По неравенству Чебышева:
\[
    P\left(\abs{\xi_n\geq A}\right) \leq \frac{ \mathbb{E}  \xi ^ 2}{A} \leq \frac{C}{A}
\]
Пусть $f \ -$ ограниченная непрерывная функция и $M = \sup \abs{f}$. Из записанного неравенства Чебышева следует, что $\forall \varepsilon > 0 \exists A$: 
\[
    P\left(\abs{\xi_n} \geq A\right) \leq \varepsilon, \ P\left(\abs{\xi} \geq A\right)  \leq \varepsilon
\]
Пусть непрерывная ограниченная $f_\varepsilon$ совпадает с $f$ на $[-A, A]$, потом от $-A-1$ до $-A$ и от $A + 1$ до $A$ она будет прямой из 0 в $f\left(-A\right)$ и $f\left(A\right)$ соответственно, а дальше будет повторять этот шаблон (периодическая).
Заметим, что:
\[
    \abs{ \mathbb{E}  f_{\varepsilon}\left(\xi_n\right) -  \mathbb{E}  f\left(\xi_n\right)} < 2M\varepsilon, \ \abs{ \mathbb{E}  f_{\varepsilon}\left(\xi\right) -  \mathbb{E}  f\left(\xi\right)} < 2M\varepsilon
\]
Теперь равномерно приблизим $f_{\varepsilon}$ комбинацией $\sin$, $\cos$ (знаем с матана, что периодическую можно так приблизить). А из сходимости хар функции мы знаем, что:
\[
   \lim_{n \to \infty}  \mathbb{E}  \sin\left(\xi_n\right) =  \mathbb{E}  \sin\left(\xi\right)\]\[
   \lim_{n \to \infty}  \mathbb{E}  \cos\left(\xi_n\right) =  \mathbb{E}  \cos\left(\xi\right)
\]
То есть получаем $\lim_{n \to \infty}  \mathbb{E}  f_{\varepsilon}\left(\xi_n\right) =  \mathbb{E}  f_{\varepsilon}\left(\xi\right)$. В итоге, вспоминая те неравенства с $2M \varepsilon$ и устремляя $n \to \infty$:
\[
     \mathbb{E}  f\left(\xi\right) - 4M \varepsilon \leq \inf \lim_{n \to \infty}  \mathbb{E}  f\left(\xi_n\right) \leq \sup \lim_{n \to \infty}  \mathbb{E}  f\left(\xi_n\right) \leq  \mathbb{E}  f\left(\xi\right) + 4M \varepsilon
\]
Устремляя $\varepsilon \to 0$, получаем $\lim_{n \to \infty}  \mathbb{E}  f\left(\xi_n\right) =  \mathbb{E}  f\left(\xi\right)$, что доказывает сходимость по распределению.
\end{proof}

\begin{corollary}
$\varphi_{\xi} \equiv \varphi_{\eta} \Rightarrow F_{\xi} \equiv F_\eta$
\end{corollary}
\begin{proof}
Предыдущая теорема + Теорема 1.
\end{proof}

\begin{theorem}(Центральная предельная теорема) \\
Пусть $\xi_n \ -$ последовательность независимых одинаково распределенных случайных величин, причем $ \mathbb{E}  \xi_1 = \mu$, $ \mathbb{D} \xi_1 = \sigma ^ 2$. Тогда $\forall x$:
\[
    \lim_{n \to \infty} P\left(\frac{\xi_1 + \cdots +\xi_n - n \mu}{\sigma \sqrt{n}} \leq t\right) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{t} e ^ {-\frac{x ^ 2}{2}}dx
\]
(справа записана $F\left(t\right) \ - $ функция распределения случайной величины с распределением $N\left(0, 1\right)$)

\end{theorem}
\begin{proof}
Переходя к случайным величинам $\xi_{i} = \frac{\xi_{i} - \mu}{\sigma}$ дальше будем считать, что $ \mathbb{E}  \xi_{i} = 0$ и $ \mathbb{D} \xi_{i} = 1$. Пусть $\varphi \ - $ хар функция случаной величины $\xi_1$. Тогда хар функция случаной величины 
\[
    \frac{\xi_1 + \cdots + \xi_n}{\sqrt{n}}
\]
равна 
\[
    \varphi_{n}\left(t\right) = \left(\varphi\left(\frac{t}{\sqrt{n}}\right)\right) ^ n
\]
Разложим $\varphi\left(\frac{t}{\sqrt{n}}\right)$ в ряд Тейлора в 0 (при $n \to \infty$), помним предложение 3:
\[
    \varphi\left(\frac{t}{\sqrt{n}}\right) = \varphi\left(0\right) + x\varphi'\left(0\right) + \cdots = 1 + 0 -\frac{t ^ 2}{2n} + o\left(\frac{1}{n}\right)
\]
Следовательно получаем:
\[
    \varphi_{n}\left(t\right) = \left(1 -\frac{t ^ 2}{2n} + o\left(\frac{1}{n}\right)\right) ^ n = \left(1 -\frac{t ^ 2}{2n} + o\left(\frac{1}{n}\right)\right) ^ {-\frac{2n}{t ^ 2} \cdot \left(-\frac{t ^ 2}{2}\right)} \xrightarrow{n \to \infty} e ^ {-\frac{t ^ 2}{2}}
\]
Получили характеристическую функцию нормального распределения, то есть и функции распределения должны совпадать.
\end{proof}


\clearpage

\section{Неравенство типа Хефдинга-Чернова}

\begin{theorem} (Неравенство Хефдинга-Чернова) \\
Пусть случайные величины $\xi_1, \cdots \xi_n$ независимы и $a_i \leq \xi_i \leq b_i$. Тогда для случайной величины $S_n = \xi_1 + \cdots + \xi_n$ и для каждого $t > 0$ выполнено
$$P\left(\abs{S_n -  \mathbb{E}  S_n} \geq t\right) \leq 2 \exp \left(-\frac{t ^ 2}{4 \sum_{i=1}^n \left(b_i - a_i\right) ^ 2}\right)$$
\end{theorem}
\begin{proof}
Пусть $\eta_i = \xi_i -  \mathbb{E}  \xi_i$, тогда $\abs{\eta_i} \leq b_i - a_i$. Заметим, что для каждого $\lambda > 0$ (просто домножили и взяли экспоненту):
$$P\left({S_n -  \mathbb{E}  S_n} \geq t\right) = P\left(\sum_{i = 1}^{n} \eta_i \geq t\right) = P\left(e ^ {\lambda \sum \eta_i} \geq e ^ {\lambda t}\right)$$
Теперь применим неравенство Маркова:
$$P\left(e ^ {\lambda \sum \eta_i} \geq e ^ {\lambda t}\right) \leq e ^ {-\lambda t}  \mathbb{E}  e ^ {\lambda \sum \eta_i}$$
Вспомним, что $\eta_1, \cdots \eta_n$ независимы:
$$ e ^ {-\lambda t}  \mathbb{E}  e ^ {\lambda \sum \eta_i} = e ^ {-\lambda t} \prod  \mathbb{E}  e ^ {\lambda \eta_i}$$
Оценим каждый множитель $ \mathbb{E}  e ^ {\lambda \eta_i}$ отдельно. Разложим его в ряд Тейлора:
$$ \mathbb{E}  e ^ {\lambda \eta_i} = 1 + \lambda  \mathbb{E}  \eta_i + \lambda ^ 2  \mathbb{E}  \eta_i ^ 2 + \sum_{k = 3}^{\infty} \frac{1}{k!}\lambda ^ k \eta ^ k \leq 1 + \lambda ^ 2 \left(b_i - a_i\right) ^ 2 + \sum_{k =3}^{\infty}\frac{1}{k!}\lambda ^ k \left(b_i - a_i\right) ^ k$$
Докажем, что при $R > 0$:
$$1 + \frac12 R ^ 2 + \sum_{k =3}^{\infty}\frac{1}{k!} R ^ k \leq e ^ {R ^ 2}$$
Если $R > 1$:
$$1 + \frac12 R ^ 2 + \sum_{k =3}^{\infty}\frac{1}{k!} R ^ k = 1 + \frac12 R ^ 2 + \sum_{k=2}^{\infty} \frac1{k!} R^{2k} \left(\frac{k!}{\left(2k-1\right)!} R ^ {-1} + \frac{n!}{\left(2n\right)!}\right) \leq 1 + \frac12 R ^ 2 + \sum_{k=2}^{\infty} \frac1{k!}R ^ {2k} = e ^ {R ^ 2}$$
Если же $R \leq 1$:
$$1 + \frac12 R ^ 2 + \sum_{k =3}^{\infty}\frac{1}{k!} R ^ k \leq 1 + \frac12 R ^2 + \sum_{k=3}^{\infty} \frac{1}{2 ^ {k - 1}} R ^ 2 = 1 + R ^ 2 \leq e ^ {R ^ 2}$$
Таким образом:
$$P\left({S_n -  \mathbb{E}  S_n} \geq t\right) \leq \exp \left(-\lambda t + \lambda ^2 \sum_{i=1}^{n}\left(b_i - a_i\right) ^ 2\right)$$
Взяв $\lambda = \frac{t}{2 \sum \left(a_i - b_i\right) ^ 2}$ получим желаемое неравенство. Для $P\left({S_n -  \mathbb{E}  S_n} \geq t\right)$ получим такую же оценку, и, объединяя их, получим оценку на модуль, только придется домножить оценку на 2.
\end{proof}

\begin{corollary}
Пусть $\xi_i \sim Bern\left(p\right) \ -$ набор $n$ независимых случайных величин, \\ $S_n = \xi_1 + \cdots + \xi_n$, тогда 
$$P\left(\abs{\frac{S_n}{n} - p} \geq t\right) \leq 2 e ^ {-\frac{n t ^ 2}{4}}$$ 
\end{corollary}
\begin{proof}
Разделим каждую случайную величину на $n$, тогда $ \mathbb{E}  \frac{S_n}{n} = p$, а $\sum_{i = 1}^{n} \left(a_i - b_i\right) ^ 2 = n \cdot \frac1{n ^ 2} = \frac1n$. Подставляем и получаем, нужное неравенство
\end{proof}

\begin{example}
Пусть в ящике какое-то количество черных и белых шаров. Каким должен быть размер выборки, чтобы оценить долю белых шаров с малой погрешностью? Пусть $\xi_i \ -$ бернуллевская случайная величина, равная 1, если шар белого цвета и 0, если цвет черный. Мы хотим оценить вероятность успеха $p$. По неравенству выше:
$$P\left(\abs{\frac{S_n}{n} - p} \geq t\right) \leq 2e^{-\frac{nt^ 2}{4}} \leq \varepsilon$$
Тогда при размере выборки $n = O\left(\frac{\ln \frac1\varepsilon}{t ^ 2}\right)$ выборочное среднее приближает реальную долю белых шаров с точностью $t$ с вероятностью более $1 - \varepsilon$(то есть вероятность, что наша оценка верна $ \geq 1 - \varepsilon$)
\end{example}

\clearpage

\section{Теоремы непрерывности}
Для применения ЦПТ на практике важную роль играют так называемые теоремы о непрерывности.

\begin{advice}
Если последовательность случайных величин $\xi_n \xrightarrow{\text{d}} \xi$, то для всякой непрерывной $g$ $g\left(\xi_n\right) \xrightarrow{\text{d}} g\left(\xi\right)$
\end{advice}
\begin{proof}
Следует из Теоремы 1(эквивалентное определение сходимости по распределению).
\end{proof}

\begin{lemma}
Пусть $X, Y, Z \ - $ случайные величины. Тогда 
$$P\left(X + Y \leq t\right) \leq P\left(X + Z \leq t + \varepsilon\right) + P\left(\abs{Y - Z} \geq \varepsilon \right), \ \forall t \in \mathbb{R}, \forall \varepsilon > 0$$
\end{lemma}
\begin{proof}
Заметим, что
$$P\left(X + Y \leq t\right) \leq  P\left(X + Y \leq t, \ \abs{Y - Z} \leq \varepsilon\right) + P\left(X + Y \leq t, \ \abs{Y - Z} \geq \varepsilon \right) \leq$$
$$ \leq  P\left(X + Z - \varepsilon \leq t \right) + P\left(\abs{Y - Z} \geq \varepsilon\right)$$
\end{proof}

\begin{advice}
Если $\xi_{n} \xrightarrow{\text{P}} a = const$ и $\eta_n \xrightarrow{\text{d}} \eta$, то $\xi_n \eta_n \xrightarrow{\text{d}} a \eta$,  $\xi_n + \eta_n \xrightarrow{\text{d}} a + \eta$
\end{advice}
\begin{proof}
Докажем утверждение для суммы. Пусть $\varepsilon > 0$, тогда по лемме имеем:
$$P\left(\xi_n + \eta_n \leq t\right) \leq F_{\eta_n}\left(t - a + \varepsilon\right) + P\left(\abs{\xi_n- a} \geq \varepsilon\right)$$
и
$$P\left(\xi_n + \eta_n \leq t\right) \geq F_{\eta_n}\left(t - a - \varepsilon\right) - P\left(\abs{\xi_n- a} \geq \varepsilon\right)$$
Устремляя сначала $n \to \infty$, а затем $\varepsilon \to 0$. Из сходимости по вероятности получаем, что $P\left( \abs{\xi_n - a} \geq \varepsilon\right) \to 0$. Тогда в итоге имеем:
    $$\lim \left(F_{\eta_n}\left(t - a - \varepsilon\right) - P\left(\abs{\xi_n- a} \geq \varepsilon\right)\right) \leq \lim F_{\xi_n + \eta_n}\left(t\right) \leq \lim \left(F_{\eta_n}\left(t - a + \varepsilon\right) + P\left(\abs{\xi_n- a} \geq \varepsilon\right)\right)$$
    $$\lim F_{\xi_n + \eta_n}\left(t\right) = F_{\eta}\left(t - a\right) = F_{a + \eta}\left(t\right)$$
Теперь докажем утверждение для произведения. Пусть $a = 0$ (случай, когда $a \neq 0$ выводится из суммы 
$\left(\xi_n - a\right)\eta_n$ и $a \eta_n$). Теперь заметим, что $\forall \varepsilon > 0 \forall C > 0$
верно включение:
 $$\left\{\abs{\xi_n \eta_n} > \varepsilon\right\} \subseteq \left\{\abs{\eta_n} > C\right\} \bigcup \left\{\abs{\xi_n} > \frac{\varepsilon}{C}\right\}$$
 (Пояснение: это верно, так как пересечение отрицания обоих событий точно приводит к противоречию). Тогда, переходя к вероятностям, получаем:
 $$P\left(\abs{\xi_n \eta_n} > \varepsilon\right) \leq 1 - F_{\eta_n}\left(C\right) + F_{\eta_n}\left(-C\right) + P\left(\abs{\xi_n}> \frac\varepsilon C\right)$$
Устремляя сначала $n \to 0$, а затем $C \to \infty$, получаем, что $\xi_n \eta_n \xrightarrow{\text{P}} 0$ $\Rightarrow$ $\xi_n \eta_n \xrightarrow{\text{d}} 0$. 
\end{proof}

\begin{example}(Выборочная дисперсия)\\
Пусть задана последовательность независимых и одинаково распределенных случайных величин $\xi_i$, причем $ \mathbb{E}  \xi_i = \mu$ и $ \mathbb{D} \xi_i = \sigma^ 2$. Тогда последовательность случайных величин
$$s_n ^ 2 = \frac1{n-1} \sum_{i = 1}^{\infty} \left(\xi_i - \overline{\xi_n}\right) ^ 2 \xrightarrow{\text{P}} \sigma ^ 2$$
где $\overline{\xi_n} = \frac{\xi_1 + \cdots+ \xi_n}{n}$ (умножаем на $\frac1{n-1}$, а не на $\frac1n$, чтобы $ \mathbb{E}  s_n ^ 2 = \sigma ^ 2$, то есть таким образом посчитанная диспресия по вероятности сходится к именно тому, чему и надо, в другом случае будет небольшое смещение в $\frac{n - 1}{n}$ раз, но с увеличение $n$ разница в любом случае будет стрираться). Действительно:
$$s_n ^ 2 = \frac1{n - 1} \left(\sum_{i = 1}^{\infty} \xi_i ^ 2 - 2 n\overline{\xi_n} \cdot \frac1n \sum_{i = 1}^{\infty} + n \overline{\xi_n} ^ 2\right) = \frac1{n - 1} \left(\sum_{i = 1}^{\infty} \xi_i ^ 2 - n \overline{\xi_n} ^ 2\right) = \frac{n}{n - 1} \cdot \frac1n \sum_{i = 1}^{\infty} \xi_i ^ 2 - \frac{n}{n - 1} \overline{\xi_n} ^ 2$$
Теперь заметим, что по ЗБЧ:
$$\frac1n \sum_{i = 1}^{\infty} \xi_i ^ 2 \xrightarrow{\text{P}} \sigma ^ 2 + \mu, \ \overline{\xi_{n}} \xrightarrow{\text{P}} \mu$$
Получили искомую сходимость
\end{example}

\begin{example}
Пусть задана последовательность независимых и одинаково распределенных случайных величин $\xi_i$, причем $ \mathbb{E}  \xi_i = \mu$ и $ \mathbb{D} \xi_i = \sigma ^ 2 > 0$. Тогда из ЦПТ следует, что:
$$\frac{\xi_1 + \cdots + \xi_n- n \mu}{\sigma \sqrt{n}} = \frac{\sqrt{n}\left(\overline{\xi_n} - \mu\right)}{\sigma} \xrightarrow{\text{d}} \xi \sim N\left(0, 1\right)$$
Более того, так как $s_n ^ 2 \xrightarrow{\text{P}} \sigma ^ 2 > 0$, то имеет место сходимость по распределению величин:
$$\frac{\sqrt{n}\left(\overline{\xi_n} - \mu\right)}{s_n ^ 2} \xrightarrow{\text{d}} \xi \sim N\left(0, 1\right)$$

\end{example}

\begin{advice}
Пусть $a, h_n \in \mathbb{R}$, $\lim_{n \to \infty} h_n = 0$ и $f \ -$ непрерывно дифференцируемая функция на $\mathbb{R}$.
Если последовательность случайных величин $\xi_n$ сходится по распределению к $\xi$, то:
\[
    \frac{f\left(a + h_n \xi_n\right) - f\left(a\right)}{h_n} \xrightarrow{d} f'\left(a\right) \xi
\]
\end{advice}

\begin{proof}
Имеет место равенство:
\[
    \frac{f\left(a + h_n \xi_n\right) - f\left(a\right)}{h_n} =
    \frac{f\left(a + 1\cdot h_n \xi_n\right) - f\left(a + 0 \cdot h_n \xi_n\right)}{h_n} = 
    \frac1{h_n} \int_{0}^{1} f\left(a + t h_n \xi_n\right) d\left(a + t h_n \xi_n\right) =
    \]
    \[
    = \xi_n \int_0^1 f\left(a + t h_n \xi_n\right) dt
\]
Из предложения 5(самый конец) получаем, что $h_n \xi_n \xrightarrow{P} 0$.
Также заметим, что функция
\[
    g\left(y\right) = \int_0^1 f(a + ty) dt
\]
непрерывна. Следовательно по предложению 4:
\[
    g\left(h_n \xi_n\right) = \int_0^1 f\left(a + t h_n \xi_n\right) dt \xrightarrow{P} g(0) = f'\left(a\right)
\]
Теперь снова используя предложение 5, получаем нужную сходимость.
\end{proof}

\begin{example}
Пусть задана последоватеность независисмых и одинаково распределенных случайных величин $\xi_i$, причем 
$ \mathbb{E}  \xi_i = \mu$ и $ \mathbb{D} \xi_i = \sigma ^ 2 > 0$. Если $h \ - $ непрерывно дифференцируемая функция, то
\[
    \sqrt{n}  \left(h\left(\overline{\xi_n}\right) - h\left(\mu\right)\right) \xrightarrow{d} \xi \sim N\left(0, q ^ 2\right), \ q = \sigma h'\left(\mu\right)
\]
Действительно, имеем равенство
\[
    \sqrt{n} \frac{\left(h\left(\overline{\xi_n}\right) - h\left(\mu\right)\right)}{\sigma} = 
    \frac1{\sigma} \cdot \frac{h\left(\mu + n ^ {-\frac12} \sigma \zeta_n\right) - h\left(\mu\right)}{n ^ {-\frac12}}
\]
где(сходимость по ЦПТ)
\[
    \zeta_n = \frac{\sqrt{n} \left(\overline{\xi_n}- \mu\right)}{\sigma} \xrightarrow{d} \xi \sim N\left(0, 1\right)
\]
Используем предложение 6 и получаем требуемое.
\end{example}

\begin{example}
Пусть $\xi_1, \cdots \xi_n$ положительные независимые одинаково распределенные случайные величины, 
$ \mathbb{E}  \xi_1 = \mu$, $0 <  \mathbb{D} \xi_1 = \sigma ^ 2 < \infty$.
Рассмотрим случайную величину $S_n = \xi_1 + \cdots + \xi_n$ и найдем предел в смысле сходимости по распределению у последовательности случайных величин
$\sqrt{n} \left(\frac{n}{S_n} - \frac1\mu\right)$.\\
Первый способ:
\[
    \sqrt{n} \left(\frac{n}{S_n} - \frac1\mu\right) = -\frac1\mu \frac{n}{S_n} \sqrt{n} \left(\frac{S_n}{n} - \mu\right)
\]
По ЦПТ
\[
    \sqrt{n} \left(\frac{S_n}{n} - \mu\right) \xrightarrow{d} \xi \sim N\left(0, \sigma ^ 2\right)
\]
По ЗБЧ
\[
    \frac{n}{S_n} \xrightarrow{P} \frac1\mu
\]
Таким образом имеем:
\[
    \sqrt{n} \left(\frac{n}{S_n} - \frac1\mu\right) \xrightarrow{d} -\frac1\mu ^2  \xi \sim N\left(0, \frac{\sigma ^ 2}{\mu ^ 4}\right)
\]
Второй способ: \\
Пусть $h\left(x\right) = \frac1x$, тогда 
\[
    \sqrt{n} \left(\frac{n}{S_n} - \frac1\mu\right) = \sqrt{n} \left(h\left(\frac{S_n}{n}\right) - h\left(m\right)\right)
    \xrightarrow{d} \xi \sim N\left(0, \sigma ^2 \left(h'\left(\mu\right)\right) ^ 2\right) = N\left(0, \frac{\sigma ^ 2}{\mu ^ 4}\right)
\]

\end{example}

\clearpage

\section{Многомерная характеристическая функция и ЦПТ}

\begin{definition}
$\textit{Характеристическая функция случайного вектора}$ $\xi = \left(\xi_1, \cdots, \xi_m\right) ^ T$ определяется равенством
\[
    \varphi_{\xi}\left(x\right) =  \mathbb{E}  e ^ {i  x \xi } = \mathbb{E} e ^ {i \sum_{i = 1}^{m} x_i \xi_i}
\]

\end{definition}

\begin{advice}
$\varphi_{\xi} \equiv \varphi_\eta$ $\Leftrightarrow$ $\xi$ и $\eta$ имеют одинаковые распределения
\end{advice}
\begin{proof}
Заметим, что
\[
    F_{\xi}\left(x_1, \cdots, x_m\right) =  \mathbb{E}  I_{\leq x_1}\left(\xi_1\right) \cdots I_{\leq x_m}\left(\xi_m\right)
\]
По аналогии с одномерным случаем, нам достаточно доказать, что 
\[
     \mathbb{E}  g_1 \left(\xi_1\right) \cdots g_m\left(\xi_m\right) =  \mathbb{E}  g_1 \left(\eta_1\right) \cdots g_m \left(\eta_m\right)
\]
для непрерывных периодических функций $g_k\left(u\right)$. Такие функции приближаются линейными комбинациями функций вида $e ^ {i \mu_k u}$. Значит, достаточно проверять совпадение выражений вида
\[
     \mathbb{E}  \exp \left(i \mu_1 \xi_1 + \cdots + i \mu_m \xi_m\right) =  \mathbb{E}  \exp \left(i \mu_1 \eta_1 + \cdots + i \mu_m \eta_m\right)
\]
А это у нас есть(это хар функции).
\end{proof}

\begin{corollary}
Случайные величины $\xi_1, \cdots, \xi_m$ независимы тогда и только тогда, когда
\[
    \varphi_{\xi}\left(y_1, \cdots , y_m\right) = \varphi_{\xi_1}\left(y_1\right) \cdots \varphi_{\xi_m}\left(y_m\right)
\]
\end{corollary}
\begin{proof}
$\Rightarrow$
\[
    \varphi_{\xi}(y_1,\cdots,y_m) = \mathbb{E}e^{i(\xi_1 y_1+\cdots+\xi_m y_m)} = \mathbb{E}e^{i
    \xi_1 y_1} \cdots e^{i\xi_m y_m} =
\]
В силу независимости $\xi_i$
\[
    = \mathbb{E}e^{i
    \xi_1 y_1} \cdots \mathbb{E}e^{i\xi_m y_m} =  \varphi_{\xi_1}\left(y_1\right) \cdots \varphi_{\xi_m}\left(y_m\right)
\]
$\Leftarrow$\\
    Сделаем новый вектор $(\eta_1, \cdots , \eta_m)$, так что:\\
    \begin{enumerate}
        \item Распр $\eta_i$ = распр $\xi_i$ \text{ }$\forall i \in \{1, \cdots , m\}$
        \item $\eta_1, \cdots, \eta_m$  независимы
    \end{enumerate}
    Определим
    $F(y) = F_{\eta_1}(y_1) \cdots F_{\eta_m}(y_m)$. Тогда мы знаем, что существует вектор, у которого такая функция распределения, из чего непременно следует независимость $\eta_1, \cdots, \eta_m$\\
    Посчитаем хар. функцию $\eta = (\eta_1, \cdots, \eta_m)$
    \[
        \varphi_{\eta}(y) = 
        \left[\begin{array}{ll}
        \text{в силу}\\
        \text{нез - сти $\eta_i$}\\
        \end{array}
        \right] = \varphi_{\eta_1}(y_1) \cdots \varphi_{\eta_m}(y_m) = \varphi_{\xi_1}(y_1) \cdots \varphi_{\xi_m}(y_m) = \varphi_{\xi}(y)
    \]
    По предыдущему предложению и независимости $\eta_1, \cdots, \eta_m$ $\Rightarrow$ $\xi_1, \cdots, \xi_m$ независимы. 
\end{proof}

\begin{theorem}
Пусть последовательность независимых одинаково распределенных случайных векторов $\xi^n = \left(\xi^n_1, \cdots, \xi^n_m\right) \in \mathbb{E}$ имеют конечные
\[
    \mathbb{E} \xi_i^1 = \mu_i, \ r_{ij} = \text{cov} \left(\xi_i^1, \xi_j^1\right)
\]
Тогда величины
\[
    \eta_i^n = \frac{\xi_i^1+ \cdots + \xi_i^n - n \mu_i}{\sqrt{n}}
\]
таковы, что последовательность векторов $\eta^n = \left(\eta_1^n, \cdots,  \eta_m^n\right) \xrightarrow{d} \eta$, характеристическая функция которого имеет вид
\[
    \varphi_{\eta}\left(y\right) = \exp \left(-\frac{ \left<yR, y \right>}{2}\right), \ R = \left(r_{ij}\right)
\]
\end{theorem}

\begin{proof}
В векторной форме:
\[  
    \eta^{n} = \dfrac{\sum\limits_{s=1}^{n}\xi^{s} - \mu n}{\sqrt{n}}
\]
Запишем хар. функцию:
\[
    \varphi_{\eta^{n}}(t) = \varphi_{(\xi^{1} - \mu + \cdots + \xi^{n} - \mu)/\sqrt{n}}(t) = \varphi_{\xi^{1} - \mu + \cdots + \xi^{n} - \mu}\left(\frac{t}{\sqrt{n}}\right) = 
    \]
    В силу независимости $\xi_i$ и их одинаковой распределенности
    \[
        = \left(\varphi_{\xi^{1} - \mu}\left(\frac{t}{\sqrt{n}}\right)\right)^n
    \]
    \begin{reminder}
    Пусть $F: \mathbb{R}^n \to \mathbb{R}^m$\\
    $x = (x_1, x_2, \cdots x_n) \in \mathbb{R}^n$ и $a = (a_1, a_2, \cdots a_n) \in \mathbb{R}^n$. Тогда ряд Тейлора функции $F$ в точке $a$ это
    \[
        F(x) = F(a) + \sum\limits_{i = 1}^{n}\dfrac{dF(a)}{dx_i}(x_i-a_i) + \dfrac{1}{2}\sum\limits_{i,j=1}^{n}\dfrac{d^2F(a)}{dx_i dx_j}(x_i-a_i)(x_j-a_j)
    \]
    \[
        + \cdots + \dfrac{1}{k!}\sum\limits_{i_1, \cdots, i_k = 1}^{n} \dfrac{d^kF(a)}{dx_{i_1}\cdots dx_{i_k}}(x_{i_1}-a_{i_1})\cdots(x_{i_k}-a_{i_k}) + R_k(x-a,a)
    \]
    \end{reminder}
    \begin{flushleft}
    Разложим в ряд Тейлора:
    \end{flushleft}
\[
    \left(\varphi_{\xi^{1} - \mu}\left(\frac{t}{\sqrt{n}}\right)\right)^n = \left(1 + \left\langle \bigtriangledown \varphi_{\xi^{1} - \mu}(0), \frac{t}{\sqrt{n}} \right\rangle + \frac{1}{2}\cdot D^2\varphi_{\xi^{1} - \mu}(0) \left<\frac{t}{\sqrt{n}}, \frac{t}{\sqrt{n}}\right>  + o\left(\frac1n\right)\right)^n
\]
Рассмотрим на 2-мерном случае (на других аналогично):
\[
    \varphi_{\xi}(t_1, t_2) = \mathbb{E}e^{it_1\xi_1 + it_2 \xi_2}
\]
Первая производная:
\[
    \frac{d}{d t_{j}} \varphi_{\xi}(t) = i\mathbb{E}\xi_{j} e^{it_1\xi_1 + it_2 \xi_2}; \text{ } \frac{d}{d t_{j}} \varphi_{\xi}(0) = i\mathbb{E}\xi_{j}
\]
В нашем случае нетрудно понять, что $\bigtriangledown \varphi_{\xi^{1} - \mu}(0) = 0$, так как у нас $\xi$ это $\xi^{1} - \mu$, а $i\mathbb{E}(\xi_{i}^{1} - \mu_{i}) = 0$\\
Вторая производная:
\[
    \dfrac{d^2}{d t_{j} t_{s}} \varphi_{\xi}(t) = -\mathbb{E}\xi_{j} \xi_{s} e^{it_1\xi_1 + it_2 \xi_2}; \text{ } \frac{d^2}{d t_{j} t_{s}} \varphi_{\xi}(0) = -\mathbb{E}\xi_{j}\xi_{s}
\]
В нашем случае $-\mathbb{E}(\xi_{j}^{1} - \mu_{j})(\xi_{s}^{1} - \mu_{s}) = -r_{js}$\\
Получаем:
\[
    \left(1 - \dfrac{1}{2}\langle Rt, t \rangle \frac{1}{n} + o\left(\frac1n\right)\right)^n \xrightarrow{n \to \infty} e^{-\frac{1}{2}\langle Rt, t \rangle}
\]
\end{proof}

\clearpage

\section{Многомерное нормальное распределение}  

\begin{definition} Случайный вектор $\xi \in \text{Mat}_{m \times 1}$ имеет $\textit{нормальное распределение}$ или является $\textit{гауссовским}$, если $\forall x \in \mathbb{R} ^ m$
\[
    \varphi_{\xi}\left(x\right) = \mathbb{E}  e ^ {i  x \xi} = e ^ {i x\mu - \frac12 x R x ^ T}
\]
где $\mu = \left(\mu_1, \cdots , \mu_m\right)^T$, $R \in \text{Mat}_{m \times m}\ - $ симметричная неотрицательно определенная. Далее кратко пишем
\[
    \xi \sim N\left(\mu, R\right)
\]
\end{definition}

\begin{definition}
Пусть $\xi \in \text{Mat}_{m \times 1}$ случайный вектор. Матрица $R \in \text{Mat}_{m \times m}$ с компонентами $r_{ij} = \text{cov} \left(\xi_i, \xi_j\right)$
называется $\textit{ковариационной матрицей}$ вектора $\xi$. Можно еще написать, что 
\[
    R = \text{cov} \left(\xi, \xi\right) = \mathbb{E} \left(\xi - \mathbb{E} \xi\right) \left(\xi - \mathbb{E} \xi\right)^T
\]
\end{definition}
\begin{lemma}
Симметричная неотрицательно определенная матрица $R \in \text{Mat}_{m \times m}$ является ковариционной матрицей случайного вектора-столбца $\xi \in \text{Mat}_{m \times 1}$ тогда и только тогда, когда $\forall x, y \in \mathbb{R} ^ m$
\[
    \text{cov} \left(x\xi, y \xi\right) = x \text{cov} \left(\xi, \xi\right) y ^ T = x R y ^ T
\]
\end{lemma}

\begin{proof}
$\Rightarrow$ \\
Распишем по определению ковариации двух случайных величин (у нас именно они):
\[
    \text{cov} \left(x\xi, y \xi\right) = \mathbb{E} \left[\left(x \xi - \mathbb{E} x \xi\right) \left(y \xi - \mathbb{E} y \xi \right)\right] = \mathbb{E} \left[x \left(\xi - \mathbb{E} \xi\right) y \left(\xi - \mathbb{E} \xi\right)\right]
     = 
\]
Транспонируя скаляр, получаем тот же скаляр:
\[
    = \mathbb{E} \left[x \left(\xi - \mathbb{E} \xi\right) \left(y \left(\xi - \mathbb{E} \xi\right)\right) ^ T\right] 
    = \mathbb{E} \left[x \left(\xi - \mathbb{E} \xi\right)\left(\xi - \mathbb{E} \xi\right) ^ T y ^ T\right] 
    = x \mathbb{E} \left[\left(\xi - \mathbb{E} \xi\right)\left(\xi - \mathbb{E} \xi\right) ^ T\right] y ^ T
    = x \text{cov} \left(\xi, \xi\right) y ^ T
\]

$\Leftarrow$ \\
Возьмем $x = e_i$, $y = e_j$ (базисные единичные вектора). Тогда из данного равенства получим:
\[
    \text{cov} \left(e_i \xi, e_j \xi\right) = \text{cov} \left(\xi_i, \xi_j\right) = e_i R e_j = r_{ij}
\]
Следовательно $R = \text{cov} \left(\xi, \xi\right)$ по определению.

\end{proof}

\begin{advice}
$\forall A \in \text{Mat}_{m \times m}$ $\forall b \in \text{Mat}_{m \times 1}$ и $\xi \in \text{Mat}_{m \times 1} \ - $  случайного вектора, верно:
\[
    \text{cov} \left(A\xi + b, A \xi + b\right) = A R_\xi A ^ T
\]

\end{advice}
\begin{proof}
Распишем по определению: 
\[
    \text{cov} \left(A\xi + b, A \xi + b\right) = \mathbb{E} \left[\left(A \xi + b - \mathbb{E} \left(A\xi + b\right)\right) \left(A \xi + b - \mathbb{E} \left(A\xi +b\right)\right) ^ T\right] = 
\]
\[
    = \mathbb{E} \left[\left(A \xi + b - b - \mathbb{E} A\xi\right) \left(A \xi + b -b -  \mathbb{E} A\xi\right) ^ T\right] = \mathbb{E} \left[\left(A \xi  - \mathbb{E} A\xi\right) \left(A \xi  -  \mathbb{E} A\xi\right) ^ T\right] = 
    \]\[
    = A \mathbb{E} \left[\left(\xi  - \mathbb{E}\xi\right) \left(\xi  -  \mathbb{E}\xi\right) ^ T\right] A ^ T
    = A R_\xi A ^ T
\]
\end{proof}
\begin{corollary}
Если вектор $\xi \sim N\left(\mu, R\right)$, то вектор $A\xi + b \sim N\left(A\mu + b, A R_\xi A ^ T\right)$
\end{corollary}

\begin{theorem}
Вектор $\xi \in \text{Mat}_{m \times 1}$ имеет нормальное распределение тогда и только тогда, когда $\forall x \in \mathbb{R} ^ m$ случайная величина $ x\xi$ имеет нормальное распределение
\end{theorem}
\begin{proof}
$\Rightarrow$ \\
Если $\xi$ нормальный вектор, то 
\[
    \varphi_{x \xi} \left(t\right) = \mathbb{E} e ^ {it  x \xi } 
     = \varphi_{\xi}\left(tx\right) = \exp\left(-\frac12  txR \left(tx\right) ^ T + i  tx\mu\right) = 
\]
\[
    = \exp\left(-\frac12 t ^ 2 xR x^ T + it x \mu\right)
\]
Получили хар функцию нормального распределения $ x \xi \sim N\left(x \mu,  xR x^ T\right)$.\\
$\Leftarrow$ \\
В обратную сторону:
\[
    \varphi_{\xi}\left(x\right) = \mathbb{E} e ^ {i  x \xi } = \varphi_{ x \xi }\left(1\right) = \exp\left(-\frac12 \mathbb{D}  x \xi  + i\mathbb{E}  x \xi \right) = \exp\left(-\frac12  \text{cov} \left( x \xi ,  x \xi \right) + i  x \mu \right) = 
    \]\[
    = \exp\left(-\frac12   xR x^ T + i  x \mu \right)
\]
, где $R = \text{cov} \left(\xi, \xi\right)$, $\mu = \mathbb{E} \xi$. Последний переход вытекает из леммы2.
\end{proof}

\begin{corollary}
Если $\xi \sim N\left(\mu, R\right)$, то $R = \text{cov}\left(\xi, \xi\right)$, $\mu = \mathbb{E} \xi$.
\end{corollary}

\begin{corollary}
Если вектор $\xi = \left(\xi_1, \xi_2\right)$ имеет нормальное распределение и $\text{cov}\left(\xi_1, \xi_2\right) = 0$, то случайные величины $\xi_1$ и $\xi_2$ независимы.
\end{corollary}

\begin{proof}
Пусть 
\[
    \mu = \mathbb{E} \xi = \left(\mu_1, \mu_2\right)
\]
Заметим, что 
\[R = 
    \text{cov} \left(\xi, \xi\right)   = 
    \begin{pmatrix}
    \text{cov}\left(\xi_1, \xi_1\right) & 0\\
    0 & \text{cov}\left(\xi_2, \xi_2\right)
    \end{pmatrix}
\]
Теперь запишем харфункцию $\xi$:
\[
    \varphi_{\xi}\left(x_1, x_2\right) = \exp\left(-\frac 1 2 \left(x_1 ^ 2 \mathbb{D} \xi_1 + x_2 ^ 2 \mathbb{D} \xi_2 \right) + i \left(x_1 \mu_1 + x_2 \mu_2\right)\right) =
\]
\[
    = \exp\left(-\frac 1 2 x_1 ^ 2 \mathbb{D} \xi_1 + ix_1 \mu_1\right) \cdot \exp\left(-\frac 1 2 x_2 ^ 2 \mathbb{D} \xi_2 + ix_2 \mu_2\right)= \varphi_{\xi_1}\left(x_1\right) \varphi_{\xi_2}\left(x_2 \right)
\]
Теперь из следствия4 вытекает независимость $\xi_1$ и $\xi_2$
\end{proof}

\begin{corollary}
Если $\xi \sim N\left(\mu, R\right)$($\in \text{Mat}_{m \times 1}$), то $\exists A \in \text{Mat}_{m \times k}$, что $\xi = A \eta + \mu$, где $\eta = \left(\eta_1, \cdots , \eta_k\right) ^ T$, $\eta_i \ -$ независимые $N\left(0, 1\right)$ случайные величины. Причем $A A ^ T = R$
\end{corollary}
\begin{proof}
Пусть \[
    \xi' = \xi - \mu = \left(\xi_1', \cdots, \xi_m'\right)^ T
\]
Свели задачу к задачи нахождения ортонормированного базиса $\eta = \left(\eta_1, \cdots, \eta_k\right) ^ T$ в подпространстве $\langle \xi_1', \cdots, \xi_m' \rangle$ со скалярным произведением $\left(X, Y\right) = \mathbb{E} XY$. Эта задача решается методом Грама-Шмидта. Получили матрицу перехода $A$, что 
\[
    \xi - \mu = \xi' = A \eta
\]
То есть 
\[
    \xi = A \eta + \mu
\]
Осталось пояснить $A A ^ T = R$:
\[
    R = \text{cov} \left(\xi, \xi\right) = \text{cov} \left(\xi - \mu, \xi - \mu\right) = \text{cov} \left(A \eta, A \eta\right) = A \text{cov}\left(\eta, \eta\right) A^ T= A E A ^ T = A A^T
\]
$\text{cov}\left(\eta, \eta\right) = E$, так как это ортонормированный базис.
\end{proof}

\begin{theorem}
Если $\xi \sim N\left(\mu, R\right)$ (в этой теореме сделаем $\mu := \mu ^ T \in R ^ m$) и $\text{det}R \neq 0$, случайный вектор $\xi$ имеет плотность
\[
    \rho\left(x\right) = \frac1 {\left(2 \pi\right) ^ {\frac m 2} \sqrt{\text{det} R}}e ^ {-2 ^ {-1} \left(x - \mu\right) R ^ {-1} \left(x - \mu\right) ^ T}
\]
\end{theorem}

\begin{proof}
Так как $\xi = A \eta + \mu$, причем $\exists A ^ {-1}$, то
\[
    P\left(\xi \in B\right) = P\left(A \eta + \mu \in B\right) = \frac{1}{\left(\sqrt{2 \pi}\right) ^ m}
    \int_{A \eta + \mu \in B} e ^ {-2 ^ {-1}x x ^ T} dx = 
\]
\[
   =  \frac 1 {\left(2 \pi\right) ^ {\frac m 2}} \int_{B} e ^ {- 2 ^ {-1} \left(A ^ {-1}\left(x - \mu\right)\right)\left(A ^ {-1}\left(x - \mu\right)\right) ^ T} d\left(A ^ {-1} \left(x - \mu\right)\right)
   = \frac 1 {\left(2 \pi\right) ^ {\frac m 2} \text{det} A} \int_{B} e ^ {- 2 ^ {-1} \left(A ^ {-1}\left(x - \mu\right)\right)\left(A ^ {-1}\left(x - \mu\right)\right) ^ T} dx
\]

Остается заметить, что 
\[
    \text{det} A A ^ T = \left(\text{det} A\right) ^ 2 = \text{det} R
\]
и что
\[
    A ^ {-1}\left(x - \mu\right)\left(A ^ {-1}\left(x - \mu\right)\right) ^ T = 
    \left(x - \mu\right) A ^ {-1} \left(A ^ {-1}\right) ^ T \left(x - \mu\right) ^ T= 
    \]\[
    = \left(x - \mu\right) \left(A A ^ T\right) ^ {-1} \left(x - \mu\right) ^ T 
    = \left(x - \mu\right) R ^ {-1} \left(x - \mu\right) ^ T
\]

\end{proof}


\begin{example}
Пусть $\xi = \left(\xi_1, \cdots, \xi_n\right) ^ T$, где $\xi_i \sim N\left(0, \sigma ^ 2\right)$ и независимы между собой (или, что то же самое $\xi \sim N\left(0, \sigma ^ 2 E\right) $). Положим
\[
    \overline{\xi} = \frac{\xi_1 + \cdots + \xi_n}{n}, \ \zeta = \left(\xi_1 - \overline{\xi}\right) ^ 2
     + \cdots + \left(\xi_n - \overline{\xi}\right) ^ 2
\]
($\zeta \ - $ выборочная диспресия). Покажем, что $\overline{\xi}$ и $\zeta$ независимы. \\
Пусть $U \in \text{Mat}_{n \times n} \ - $ ортогональная матрица ($U U ^ T = E$), первая строка которой имеет вид $\left(n ^ {-\frac 1 2}, \cdots, n ^ {-\frac 1 2}\right)$. Тогда координаты вектора $u = U \xi \sim N\left(0, U \sigma ^ 2 E U ^ T\right) = N\left(0, \sigma ^ 2 E\right)$ являются независимыми. Заметим, что $u_{n}= \overline{\xi}\sqrt{n}$ и что
\[
    u ^ T u = u_1 ^ 2 + \cdots + u_n ^ 2 = n \overline{\xi ^ 2} + u_2 ^ 2 + \cdots + u_n ^ 2= \xi ^ T U ^ T U \xi = \xi ^ T \xi = \xi_1 ^ 2 + \cdots + \xi_n ^ 2
\]
Иначе говоря
\[
    u_2 ^ 2 + \cdots + u_n ^ 2 = \xi_1 ^ 2 + \cdots + \xi_n ^ 2 - n \overline{\xi ^ 2} 
\]
Теперь заметим
\[
    \zeta = \xi_1 ^ 2 + \cdots + \xi_n ^ 2 - 2 \sum_{i = 1}^{n} \xi_i \overline{\xi} + n \overline{\xi ^ 2}
    = \xi_1 ^ 2 + \cdots + \xi_n ^ 2 - n \overline{\xi ^ 2} = u_2 ^ 2 + \cdots + u_n ^ 2
\]
Так как $u_1, \cdots , u_n \ -$ независимы, то и $\frac{u_1}{\sqrt{n}} = \overline{\xi}$ и $u_2 ^ 2 + \cdots + u_n ^ 2 = \zeta$ тоже независимы. \\
Распределение величины $\chi = \eta_1^2 + \cdots + \eta_n^2$, где $\eta_i$ независимые c распределением $N\left(0, 1\right)$, называют распределением хи-квадрат с $n$ степенями свободы и обозначают через $\chi_{n} ^ 2$. 
Найдем плотсноть распределения $\chi_n ^ 2$:
\[
    P\left(\chi \leq t\right) = \left(2 \pi\right) ^ {-\frac{n}{2}} \int_{\eta_1 ^ 2 + \cdots + \eta_n ^ 2 \leq t} e ^ {-\frac{x_1 ^ 2 + \cdots + x_n ^ 2}{2}} dx = 
 \]
 Делаем сферическую замену ($w_n \ -$ площадь $n$-мерной единичной сферы):
 \[
     = \left(2 \pi\right) ^ {-\frac{n}{2}} w_n \int_{0}^{\sqrt{t}} r ^ {n - 1} e ^ {-\frac{ r ^ 2} 2} dr
 \]
 Тогда 
 \[
    \rho\left(t\right) = \left(2 \pi\right) ^ {-\frac{n}{2}} w_n \cdot \frac{1}{2} t ^ {-\frac{1}{2}} t ^ {\frac{n - 1}{2}} e ^ {-\frac{ t} 2} =  \frac{1}{2}  \left(2 \pi\right) ^ {-\frac{n}{2}} w_n  t ^ {\frac{n - 2}{2}} e ^ {-\frac{t}{2}} \text{Ind}_{t > 0}
 \]
\end{example}

\clearpage
 \section{Условные математические ожидания: дискретный случай}

Предположим, что задана дискретная случайная величина
\[
    \xi\left(w\right) = \sum_{i = 1}^{n} x_i \text{Ind}_{A_i}\left(w\right)
\]
Рассмотрим следующую задачу: найти математическое ожидание $\xi$, если достоверно известно, что произошло событие $B$, $P\left(B\right) > 0$. Поскольку мы знаем, что событие $B$ произошло, то надо пересчитать вероятности $A_k$ с учетом новой информации, а именно, заменить $P\left(A_k\right)$ на $P\left(A_k | B\right)$. Таким образом, надо вычислить математическое ожидание не относительно исходной вероятностной меры $P$, а относительно условной вероятности $P\left(\cdot | B\right)$. 
\begin{definition}
Имеем:
\[
    \mathbb{E}\left(\xi | B\right) = \sum_{i = 1}^{n} x_i P\left(A_i | B\right) =
    \sum_{i = 1}^{n} x_i \frac{\mathbb{E} \left(\text{Ind}_{A_i} \text{Ind}_{B}\right)}{P\left(B\right)} = \frac{\mathbb{E}\left(\xi \text{Ind}_{B}\right)}{P\left(B\right)}
\]
Это выражение будем называть $\textit{условным математическим ожиданием относительно события B}$.

\end{definition}

Пусть теперь имеется разбиение 
\[
    \Omega = \bigcup_{k = 1}^{N} B_k, \ B_k \bigcap B_m = \emptyset, \ P\left(B_k\right) > 0
\]
Обозначим это разбиение $\{B_k\}$ через $\mathcal{B}$. Удобно собрать вместе значения условных математических ожиданий $\mathbb{E}\left(\xi | B_k\right)$.
\begin{definition}
Рассмотрим случайную величину: 
\[
    \Lambda\left(w\right) = \sum_{i = 1}^{N} \text{Ind}_{B_i}\left(w\right) \mathbb{E} \left(\xi | B_i\right)
\]
Если $w \in B_i$, то эта случайная величина выдает среднее значение $\xi$ при условии, что произошло событие $B_i$. Величину $\Lambda\left(w\right)$ называют $\textit{условным математическим ожиданием относительно разбиения}$ $ \mathcal{B}$ и обозначают через $\mathbb{E} \left(\xi | \mathcal{B}\right)$.
\end{definition}
Случайную величину
\[
    P\left(A | \mathcal{B}\right) = \mathbb{E}\left(\text{Ind}_A | \mathcal{B}\right)
\]
называют условной вероятностью события $A$ относительно разбиения $\mathcal{B}$. Ясно, что 
\[
    \mathbb{E}\left(\xi | \mathcal{B}\right) = \sum_{i = 1}^{N} \text{Ind}_{B_i}\left(w\right) \mathbb{E} \left(\xi | B_i\right) 
    = \sum_{i = 1}^{N} \text{Ind}_{B_i}\left(w\right) \sum_{j = 1}^{n} x_j P\left(A_j | B_i\right)
    = \sum_{j = 1}^{n} \sum_{i = 1}^{N} \text{Ind}_{B_i}\left(w\right) x_j P\left(A_j | B_i\right)
    = 
\]
\[
    = \sum_{j = 1}^{n} x_j \sum_{i = 1}^{N} \text{Ind}_{B_i}\left(w\right) P\left(A_j | B_i\right)
    = \sum_{j = 1}^{n} x_j \sum_{i = 1}^{n} \text{Ind}_{B_i}\left(w\right) \mathbb{E}\left(\text{Ind}_{A_j} | B_i\right) = \sum_{j = 1}^{n} x_j \mathbb{E} \left(\text{Ind}_{A_j} | \mathcal{B}\right)= 
\]
\[
    = \sum_{j = 1}^{n} x_j P\left(A_j | \mathcal{B}\right)
\]

\begin{example}
Рассмотрим важный пример, когда $\mathcal{B} = \left\{ B, \overline{B}\right\}$. Тогда 
\[
    P\left(A | \mathcal{B}\right) = \text{Ind}_{B}\left(w\right) P\left(A | B\right) + \text{Ind}_{\overline{B}}\left(w\right) P\left(A | \overline{B}\right)
\]
Если $w \in B$, то $P\left(A | \mathcal{B}\right)\left(w\right) = P\left(A | B\right)$
\end{example}

\begin{theorem}
Имеют место следующие свойства условного математического ожидания:
\begin{enumerate}[label=(\arabic*)]
\item (линейность) $\mathbb{E}\left(\alpha \xi + \beta \eta | \mathcal{B}\right) = \alpha \mathbb{E}\left(\xi | \mathcal{B}\right) + \beta \mathbb{E}\left(\eta | \mathcal{B}\right)$
\item (монотонность) из $\xi \leq \eta$ следует $\mathbb{E}\left(\xi | \mathcal{B}\right) \leq \mathbb{E}\left(\eta | \mathcal{B}\right)$
\item (аналог формулы полной вероятности) $\mathbb{E}\left(\mathbb{E}\left(\xi | \mathcal{B}\right)\right) = \mathbb{E}\xi$
\item (независимость) если случаная величина $\xi$ не зависит от разбиения $\mathcal{B}$, т.е. случайные величины $\xi$ и $\text{Ind}_{B_k}$ независимы, то $\mathbb{E}\left(\xi | \mathcal{B}\right) = \mathbb{E} \xi$
\item для всякой случайной величины $\eta = \sum_{k = 1}^{N} c_k \text{Ind}_{B_k}$ верно равенство
$\mathbb{E}\left(\eta \xi | \mathcal{B}\right) = \eta \mathbb{E}\left(\xi | \mathcal{B}\right)$

\end{enumerate}
\end{theorem}
\begin{proof}
Свойства $(1)$ и $(2)$ следуют из того, что они верны отдельно для каждого  $B_k$. \\ 
Свойство $(3)$ проверяется непосредственной подстановкой: 
\[
    \mathbb{E}\left(\mathbb{E}\left(\xi | \mathcal{B}\right)\right) = \mathbb{E} \left(\sum_{i = 1}^{N} \text{Ind}_{B_{i}} \mathbb{E}\left(\xi | B_i\right)\right)
    = \mathbb{E} \left(\sum_{i = 1}^{N} \text{Ind}_{B_{i}} \frac{\mathbb{E}\left(\xi \text{Ind}_{B_i}\right)}{P\left(B\right)}\right) = \sum_{i = 1}^{N} \mathbb{E}\left(\text{Ind}_{B_i}\right) \frac{\mathbb{E}\left(\xi \text{Ind}_{B_i}\right)}{P\left(B_i\right)} = \]
    \[
    =\sum_{i = 1}^{N} \mathbb{E}\left(\xi \text{Ind}_{B_i}\right) = \mathbb{E} \xi
\]
Обоснуем пункт $(4)$. Так как $\xi$ и $\text{Ind}_{B_k}$ независимы, то 
\[
    \mathbb{E}\left(\xi | B_k\right) = \frac{\mathbb{E}\left(\xi \text{Ind}_{B_k}\right)}{P\left(B_k\right)} = \frac{\mathbb{E}\xi \mathbb{E} \text{Ind}_{B_k}}{P\left(B_k\right)} = \mathbb{E}\xi
\]
Следовательно,
\[
    \mathbb{E}\left(\xi | \mathcal{B}\right) = \sum_{k = 1}^{N} \text{Ind}_{B_k}\left(w\right) \mathbb{E} \left(\xi | B_k\right) = \sum_{k = 1}^{N} \text{Ind}_{B_k}\left(w\right) \mathbb{E} \xi = \mathbb{E} \xi
\]
Для обоснования $(5)$ достаточно заметить, что
\[
    \mathbb{E}\left(\eta \xi | B_k\right) = c_k \mathbb{E}\left(\xi | B_k\right)
\]
\end{proof}

Наиболее типична ситуация, когда разбиение $\mathcal{B}$ появляется посредством некоторой случайной величины
\[
    \eta = \sum_{i = 1}^{N} y_i \text{Ind}_{B_i},
\]
где $y_i \ - $ разлиные числа и $P\left(B_i\right) > 0$.
\begin{definition}
В этом случае $B_i = \left\{w : \eta\left(w\right) = y_i\right\}$ и условное математическое ожидание $\mathbb{E}\left(\xi | \mathcal{B}\right)$ обозначают через $\mathbb{E}\left(\xi| \eta\right)$ и называют \textit{условным математическим ожиданием относительно} $\eta$.

\end{definition}
Несложно предъявить функцию $F$ (это можно сделать несколькими способами), что 
\[
    \mathbb{E}\left(\xi | \eta\right)\left(w\right) = F\left(\eta\left(w\right)\right)
\]
Легко видеть, что $F\left(y_i\right) = \mathbb{E}\left(\xi | B_i\right)$.
\\
Можно воспринимать $\mathbb{E}\left(\xi | \eta\right)$ как проекцию $\xi$ на $\eta$, а $\mathbb{E} \xi \eta$ как их скаляное произведение.
\begin{lemma}
Для условного математического ожидания выполнено
\[
    \mathbb{E}\left(\xi f\left(\eta\right)\right) = \mathbb{E}\left[f\left(\eta\right)\mathbb{E}\left(\xi | \eta\right)\right]
\]
для произвольной функции $f$. Кроме того, если для какой-то случайной величины $\zeta = g\left(\eta\right)$ выполнено 
\[
    \mathbb{E}\left(\xi f\left(\eta\right)\right) = \mathbb{E}\left(f\left(\eta\right) \zeta\right),
\]
то $\zeta = \mathbb{E}\left(\xi | \eta\right)$ п.н.
\end{lemma}

\begin{proof}
По $(5)$ и $(3)$ из теоремы 11:
\[
    \mathbb{E}\left[f\left(\eta\right) \mathbb{E}\left(\xi | \eta\right)\right] =
    \mathbb{E} \left[\mathbb{E}\left(f\left(\eta\right) \xi | \eta\right)\right] = \mathbb{E}\left(f\left(\eta\right) \xi\right)
\]
Докажем вторую часть: 
\[
    \mathbb{E} \left(f\left(\eta\right) \zeta\right) = \mathbb{E}\left(\xi f\left(\eta\right)\right) = \mathbb{E}\left[f\left(\eta\right)\mathbb{E}\left(\xi | \eta\right)\right]
\]
\[
    \mathbb{E} \left[f\left(\eta\right) \left(\zeta - \mathbb{E}\left(\xi | \eta \right)\right)\right] = 0
\]
Так как $\zeta$ и $\mathbb{E}\left(\xi | \eta\right) \ -$ функции от $\eta$, то возьмем $f\left(\eta\right) = \zeta - \mathbb{E}\left(\xi | \eta \right)$ и получим:
\[
    \mathbb{E} \left(\zeta - \mathbb{E}\left(\xi | \eta \right)\right)^ 2 = 0,
\]
то есть $\zeta = \mathbb{E}\left(\xi | \eta\right)$ п.н.
\end{proof}
Теперь докажем, что $\mathbb{E}\left(\xi | \eta\right)$ и правда является проекцией $\xi$ на $\eta$.
\begin{advice}

Пусть $\mathbb{E} \xi ^ 2 < \infty$. Условное матетическое ожидание $\mathbb{E}\left(\xi | \eta\right)$ среди всех случайных величин вида $f\left(\eta\right)$ является лучшим среднеквадратическим приближением для $\xi$, т.е.
\[
    \min_{\zeta : \zeta = f\left(\eta\right)} \mathbb{E} \left(\xi - \zeta\right) ^ 2 = \mathbb{E} \left[\xi - \mathbb{E}\left(\xi | \eta\right)\right] ^ 2
\]
\end{advice}

\begin{proof}
Пусть $\zeta = f\left(\eta\right)$. Так как ($\mathbb{E} \left(\xi | \eta\right) - \zeta \ -$ функция от $\eta$) 
\[
    \mathbb{E} \left[\left(\xi - \mathbb{E}\left(\xi | \eta\right)\right)\left(\mathbb{E} \left(\xi | \eta\right) - \zeta\right)\right] = 0,
\]
то
\[
    \mathbb{E} \left(\xi - \zeta\right) ^2 = \mathbb{E} \left[\left(\xi - \mathbb{E}\left(\xi | \eta\right)\right) + \left(\mathbb{E}\left(\xi | \eta\right) - \zeta\right)\right] ^ 2
    = \]
    \[
    \mathbb{E}\left[\xi - \mathbb{E}\left(\xi | \eta\right)\right] ^ 2 + 2 \underbrace{\mathbb{E} \left[\left(\xi - \mathbb{E}\left(\xi | \eta\right)\right)\left(\mathbb{E} \left(\xi | \eta\right) - \zeta\right)\right]}_{= 0} + \mathbb{E} \left[\mathbb{E}\left(\xi | \eta\right) - \zeta\right] ^ 2 \geq \mathbb{E} \left[\xi - \mathbb{E}\left(\xi | \eta\right)\right] ^ 2
\]
Последнее неравенство достигается взятием $\zeta = \mathbb{E}\left(\xi | \eta\right)$
\end{proof}

\clearpage

\section{Условные математические ожидания: общий случай}

\begin{definition}
$\xi, \eta \ -$ случаные величины. $\mathbb{E} \abs{\xi} < \infty$. Тогда случайная величина вида $F\left(\eta\right)$ называется $\textit{условным математическим ожиданием} $ $\mathbb{E}\left(\xi | \eta\right)$, если 
\[
    \mathbb{E} \left[\xi f\left(\eta\right)\right] = \mathbb{E} \left[\mathbb{E} \left(\xi | \eta\right) f\left(\eta\right)\right]
\]
для любой ограниченной $f$. Любые две случаные величины, удовлетворяющие этому условию почти наверное совпадают (лемма 3).
\end{definition}
Из этого определения следует, что $\mathbb{E}\left(\xi | \eta\right)$  есть наименее отличающаяся от $\xi$ случайная величина вида $F\left(\eta\right)$, то есть проекция $\xi$ на $\eta$.

\begin{advice}
Предположим, что распределение случайной величины $\left(\xi, \eta\right)$ задано совместной плотностью $\rho_{\xi \eta}\left(x, y\right)$. Тогда
\[
    \mathbb{E} \left[g\left(\xi, \eta\right) | \eta = y \right] = \int_{-\infty}^{+\infty} g\left(x, y\right)\frac{\rho_{\xi \eta}\left(x, y\right)}{\rho_{\eta}\left(y\right)} dx
\]
\end{advice} 

\begin{proof}
Имеет место цепочка равенств:
\[
    \mathbb{E} \left[g\left(\xi, \eta\right) f\left(\eta\right)\right] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g\left(x, y\right) f\left(y\right) \rho_{\xi \eta}\left(x, y\right) dx dy  = \]
    \[
    = \int_{-\infty}^{+\infty} f\left(y\right) \underbrace{\int_{-\infty}^{+\infty}g\left(x, y\right)\frac{\rho_{\xi \eta}\left(x, y\right)}{\rho_{\eta}\left(y\right)}}_{= F\left(y\right)} \rho_{\eta}\left(y\right) dy = \mathbb{E} \left[F\left(\eta\right) f\left(\eta\right)\right]
\]
Следовательно $F\left(\eta\right) = \mathbb{E} \left(\xi | \eta\right)$ по определению.
\end{proof}

\begin{definition}
$\textit{Условной плотностью}$ случайной величины $\xi$ при условии $\eta = y_0$ называется следующая величина
\[
    \rho_{\xi|\eta}\left(x | y_0\right) = \frac{\rho_{\xi,\eta}\left(x, y_0\right)}{\rho_{\eta}\left(y_0\right)}
\]
\end{definition}

Теперь докажем теорему 11, только для непрерывного случая 
\begin{theorem}
Имеют место следующие свойства условного математического ожидания:
\begin{enumerate}[label=(\arabic*)]
\item (линейность) $\mathbb{E}\left(\alpha \xi + \beta \eta | \zeta\right) = \alpha \mathbb{E}\left(\xi | \zeta\right) + \beta \mathbb{E}\left(\eta | \zeta\right)$
\item (монотонность) из $\xi \leq \eta$ следует $\mathbb{E}\left(\xi | \zeta\right) \leq \mathbb{E}\left(\eta | \zeta\right)$
\item (аналог формулы полной вероятности) $\mathbb{E}\left(\mathbb{E}\left(\xi | \eta\right)\right) = \mathbb{E}\xi$
\item (независимость) если случаные величины $\xi$ и $\eta$, то $\mathbb{E}\left(\xi | \eta\right) = \mathbb{E} \xi$
\item для всякой случайной величины $\zeta = g\left(\eta\right)$ верно равенство
$\mathbb{E}\left(\zeta \xi | \eta\right) = \zeta \mathbb{E}\left(\xi | \eta\right)$

\end{enumerate}
\end{theorem}

\begin{proof}
Докажем $\left(1\right)$. По определению для любой ограниченной $f$ имеем:
\[
    \mathbb{E} \left[f\left(\zeta\right) \mathbb{E}(\alpha \xi + \beta \eta | \zeta)\right] = \mathbb{E}\left[f\left(\zeta\right) \left(\alpha \xi + \beta \eta\right)\right] = \alpha \mathbb{E}\left[f\left(\zeta\right) \xi\right] + \beta \mathbb{E}\left[f\left(\zeta\right) \eta\right] =
\]
\[
    =\mathbb{E}\left[f\left(\zeta\right)\left(\alpha \mathbb{E}\left(\xi | \zeta\right) + \beta \mathbb{E}\left(\eta | \zeta\right)\right)\right]
\]
Теперь взяв $f\left(\zeta\right) = \mathbb{E}\left(\alpha \xi + \beta \eta | \zeta\right) - \left(\alpha \mathbb{E}\left(\xi | \zeta\right) + \beta \mathbb{E}\left(\eta | \zeta\right)\right)$, получим нужное равенство почти наверное. \\
Во втором, если перенести все вправо, то по сути надо доказать
\[
    \xi \geq 0 \Rightarrow \mathbb{E}\left(\xi | \eta\right) \geq 0
\]
Возьмем функцию
\[
    f\left(\eta\right) = 1 - \text{sgn}\left(\mathbb{E}\left(\xi | \eta\right)\right) \geq 0
\]
Тогда по определению
\[
    \mathbb{E}\left[f\left(\eta\right) \mathbb{E}\left(\xi | \eta\right)\right] = \mathbb{E}\left[f\left(\eta\right) \xi\right] \geq 0
\]
В то же время 
\[
    \mathbb{E}\left[f\left(\eta\right) \mathbb{E}\left(\xi | \eta\right)\right] =
    \mathbb{E}\left[\mathbb{E}\left(\xi | \eta\right) - \abs{\mathbb{E}\left(\xi | \eta\right)}\right] \leq 0
\]
Следовательно 
\[
    \mathbb{E}\left(\xi | \eta\right) = \abs{\mathbb{E}\left(\xi | \eta\right)}
\]
почти наверное, следовательно она почти наверное $\geq 0$. \\
В $\left(3\right)$ возьмем  $f\left(\eta\right) \equiv 1$ и запишем определение. \\
$\left(4\right)$:
\[
    \mathbb{E}\left[f\left(\eta\right) \xi\right] = \mathbb{E}f\left(\eta\right) \mathbb{E} \xi = \mathbb{E}\left[f\left(\eta\right) \mathbb{E}\xi\right] \Rightarrow \mathbb{E}\left(\xi | \eta\right) = \mathbb{E}\xi
\]
$\left(5\right)$:
\[
    \mathbb{E}\left[f\left(\eta\right) \xi \zeta\right] = \mathbb{E} \left[f\left(\eta\right) \mathbb{E}\left(\xi \zeta | \eta\right)\right]
\]
С другой стороны:
\[
    \mathbb{E}\left[f\left(\eta\right) \xi \zeta\right] = \mathbb{E}\left[f\left(\eta\right) g\left(\eta\right) \zeta\right] = \mathbb{E}\left[f\left(\eta\right)g\left(\eta\right) \mathbb{E}\left(\xi | \eta\right)\right]
\]
\[
    \Rightarrow \mathbb{E}\left(g\left(\eta\right) \xi | \eta\right) = g\left(\eta\right)\mathbb{E}\left(\xi | \eta\right)
\]
почти наверное

\end{proof}

\begin{theorem} (Аналог формулы Байеса) \\
\[
    \mathbb{E}\left(g\left(\eta\right) | \xi = x\right) = \frac{\mathbb{E}\left[g\left(\eta\right) \rho_{\xi | \eta}\left(x, \eta\right)\right]}{\mathbb{E} \rho_{\xi | \eta}\left(x, \eta\right)}
\]
для любой ограниченной $g$
\end{theorem}
\begin{proof}
Для любой ограниченной $f$:
\[
    \mathbb{E}\left[f\left(\xi\right)g\left(\eta\right)\right] = \mathbb{E}\left[g\left(\eta\right)\mathbb{E}\left(f\left(\xi\right) | \eta\right)\right] = \mathbb{E}\int_{-\infty}^{+\infty} f\left(x\right) \rho_{\xi | \eta}\left(x, \eta\right)g\left(\eta\right) dx = \int_{-\infty}^{+\infty} f\left(x\right)\mathbb{E} \left[\rho_{\xi | \eta}\left(x, \eta\right)g\left(\eta\right)\right] dx
\]
С другой стороны
\[
    \mathbb{E}\left[f\left(\xi\right)g\left(\eta\right)\right] = \mathbb{E}\left[f\left(\xi\right) \mathbb{E}\left(g\left(\eta\right) | \xi\right)\right] = \int_{-\infty}^{+\infty} f\left(x\right) \rho_{\xi}\left(x\right) \mathbb{E}\left[g\left(\eta\right) | \xi = x\right] dx
\]
(опять подставляя вместо $f$ разность этих величин) получаем равенство почти наверное:
 \[
    \mathbb{E} \left[\rho_{\xi | \eta}\left(x, \eta\right)g\left(\eta\right)\right] = \rho_{\xi}\left(x\right) \mathbb{E}\left[g\left(\eta\right) | \xi = x\right]
 \]
 Подставим $g \equiv 1$:
 \[
    \mathbb{E} \left[\rho_{\xi | \eta}\left(x, \eta\right)\right] = \rho_{\xi}\left(x\right)
 \]
 В итоге получаем:
 \[
     \mathbb{E}\left[g\left(\eta\right) \rho_{\xi | \eta}\left(x, \eta\right)\right] = 
     \mathbb{E} \left[\rho_{\xi | \eta}\left(x, \eta\right)\right]\mathbb{E}\left[g\left(\eta\right) | \xi = x\right]
 \]
\end{proof}

\begin{example}
Пусть $\left(\xi, \eta\right) \ - $ нормальный вектор. Посчитаем $E\left[\xi | \eta\right]$ (помним, что это как проекция $\xi$ на $\eta$). Центрируем случайные величны
\[
    X := \xi - \mathbb{E} \xi
\]
\[
    Y := \eta - \mathbb{E} \eta
\]
Найдем ортогональную $Z$ проекцию $X$ на $Y$ (должно быть $X = Z + \mathbb{E}\left[\xi | \eta\right]$):
\[
    Z = X - \frac{\text{cov}\left(X, Y\right)}{\text{cov}\left(Y, Y\right)}Y = 
    X - \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}Y
\]
Выразим $\xi$:
\[
    \xi = \mathbb{E} \xi - \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}\left(\eta - \mathbb{E} \eta\right) + Z
\]
Теперь будем считать условное матожидание (условно от константы или $f\left(\eta\right)$ она сама, условное от независимой $-$ его ожидание):
\[
    \mathbb{E}\left[\xi | \eta\right] = \mathbb{E}\xi - \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}\left(\eta - \mathbb{E} \eta\right) + \mathbb{E} Z = \mathbb{E}\xi - \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}\left(\eta - \mathbb{E} \eta\right)
\]
Так как $\left(Y, Z\right) \ -$ нормальный вектор и $\text{cov}\left(Y, Z\right) = 0$, то $Y$ и $Z$ независимые случайные величины. $Z \sim N\left(0, \frac{\mathbb{D} \xi \mathbb{D} \eta - \left[\text{cov}\left(\xi, \eta\right)\right] ^ 2}{\mathbb{D} \eta}\right)$, так как это линейная комбинация случайных величин нормального вектора. \\
Теперь найдем условную плотность:
\[
    \mathbb{E}\left[f\left(X\right) | Y = y\right] = \mathbb{E} \left[f\left(Z + \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}Y\right) | Y = y\right] = \int_{-\infty}^{+\infty} f\left(z + \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}y\right) \frac{\rho_{Z, Y}\left(z, y\right)}{\rho_{Y}\left(y\right)} dz = 
\]
Совместная плотность раскладывается в произведение:
\[
    = \int_{-\infty}^{+\infty} f\left(z + \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}y\right) \rho_{Z}\left(z\right) dz = 
\]
Делаем замену $u = z + \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta}y$:
\[
    = \int_{-\infty}^{+\infty} f\left(u\right) \rho_Z\left(u - \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta} y\right) du
\]
В итоге получаем
\[
    \rho_{X|Y}\left(x, y\right) = \frac{1}{\sigma \sqrt{2\pi}} e ^ {-\frac{\left(x - \mu\right)}{2 \sigma ^ 2}} = \rho_{Z}\left(z \right)
\], где
\[
    \mu  = \frac{\text{cov}\left(\xi, \eta\right)}{\mathbb{D}\eta} y
\]
\[
    \sigma = \frac{\mathbb{D} \xi \mathbb{D} \eta - \left[\text{cov}\left(\xi, \eta\right)\right] ^ 2}{\mathbb{D} \eta}
\]
\end{example}

\clearpage

\begin{center}
\section*{Статистика}
\end{center}
\section{Оценки параметров и их свойства}

Пусть есть $X$ случайная величина. Мы знаем распределение случайной величины $F_{\theta}\left(t\right)$ с точностью до параметра $\theta \in \Theta$. Задача статистики заключается в том, чтобы оценить параметр $\theta$.
\begin{definition}
Вектор $\left(X_1, \cdots, X_n\right)$ с независимыми компонентами, где все случайные величины $X_i \sim X$, называется \textit{выборкой}. В теории вероятности выборка $-$ набор случайных величин, в статистике выборка $-$ величины, полученные в ходе эксперимента над этими случайными величинами (\textit{простая выборка}, $\left(x_1, \cdots, x_n\right) = \left(X_1\left(w\right), \cdots, X_n\left(w\right)\right)$).
\end{definition}

\begin{definition}
Случайные величины вида $T_N\left(X_1, \cdots, X_n\right)$ называются \textit{статистиками}. Если статистика оценивает $\theta$, то она называется оценкой. Обозначение: $\widehat{\theta}_n\left(X_1, \cdots, X_n\right)$ 
\end{definition}

Сформулируем свойства хороших оценок:
\begin{enumerate}
\item \textbf{Несмещенность}. $\mathbb{E} \widehat{\theta}\left(X_1, \cdots, X_n\right) = \theta$. Заметим, что матожидание здесь мы считаем по распределению $F_{\theta}$, то есть хочется, чтобы оценка в среднем была верна.
\begin{example}
Пусть $\mathbb{E} X_i = \mu$, $\mu \ - $ единственный параметр. Возьмем оценку $\overline{X}$. Она будет несмещенной: $\mathbb{E} \overline{X} = \mu$. А если, например, параметризована дисперсия $\sigma ^ 2$ и в качестве оценки берем смещенную выборочную дисперсию, то $\mathbb{E}s_n ^ 2 = \frac{n}{n - 1} \sigma ^ 2 \neq \sigma^2$, получили смещенную оценку.
\end{example}
\begin{example}
Пусть $X \sim Bern\left(p\right)$. Тогда для нахождения несмещенной статистики, надо чтобы выполнялось равенство
\[
    \mathbb{E} T\left(X_1, \cdots, X_n\right) = \sum_{k = 0}^{n} \left[p ^ k \left(1 - p\right) ^ {n - k} \sum_{k \ \text{успехов}}T\left(\varepsilon_1, \cdots, \varepsilon_n\right)\right] = \theta.
\]
Если мы опеределим $\theta = \frac 1 p$, то несмещенных статистик не существует. Несмещенность должна выполняться $\forall \theta$, но, если $p \to 0$, то левая часть будет стремиться к некоторой константе, а правая $\theta \to +\infty$.
\end{example}
\begin{example}
Теперь приведем пример бессмыленной несмещенной оценки. Пусть $N = 1$, $X \sim Pois\left(\theta\right)$, где $0 < \theta < 1$. Тогда
\[
    \mathbb{E} T\left(X_1\right) = e ^ {-\theta} \sum_{k = 0}^{+\infty}\frac{T\left(k\right) \theta ^ k}{k!} = \theta
\]\[
    \sum_{k = 0}^{+\infty}\frac{T\left(k\right) \theta ^ k}{k!} = \theta e ^ {\theta} = \sum_{k=0}^{+\infty} \frac{\theta ^ {k + 1}}{k!}
\]
Приравнивая соответственные члены, получаем, что $T\left(k\right) = k$, то есть $T\left(X_1\right) = X_1$. Наша оценка никак не пересекается с множеством параметров, она бессмысленна.
\end{example}

\item \textbf{Состоятельность}. Оценка называется состоятельной, если $\widehat{\theta}_n\left(X_1, \cdots, X_n\right) \xrightarrow{P} \theta$. Это значит, на больших объемах данных мы получаем более точную оценку (ЗБЧ).
\begin{statement}
Если $\widehat{\theta}_n \ - $ несмещенная оценка $\theta$ и $\mathbb{D} \widehat{\theta}_n \to 0 $, то $\widehat{\theta}_n \ - $ состоятельная.
\end{statement}
\begin{proof}
Запишем неравенство Чебышева:
\[
    P\left(\abs{\widehat{\theta} - \theta} \geq \varepsilon\right) \leq \frac{\mathbb{D} \widehat{\theta}_n}{\varepsilon ^ 2}
\]
Все ясно.
\end{proof}
Записанное неравенство наталкивает на мысль, что при маленькой дисперсии, наши оценки будут более вероятно слабо разбросаны вокруг $\theta$.
\item \textbf{Оптимальность}. Несмещенная оценка $\widehat{\theta}_n$ называется оптимальной, если $\mathbb{D} \widehat{\theta}_n \leq \mathbb{D} \theta^*_{n}$, $\forall$ несмещенных оценок $\theta^*_n$
\begin{statement}
Не существует в общем случае самой оптимальной оценки в множестве всех оценок.
\end{statement}
\begin{proof}
Допустим, такая оценка существует, что
\[
    \mathbb{E}\left[\widehat{\theta}_n - \theta\right] ^ 2 \leq \mathbb{E}\left[\theta^*_n - \theta\right] ^ 2
\]
для $\forall$  $\theta \in \Theta$. Но если у нас, например, $\theta = A$ и мы возьмем $\theta^*_n \equiv A$, то получим, что $\widehat{\theta}_n = A$ почти наверное по распределению $F_A$. Теперь возьмем, допустим, что $\theta = B$ и возьмем $\theta^*_n \equiv B \neq A$ и получим, что $\widehat{\theta}_n = B$ почти наверное по распределению $F_B$. Получили противоречие (противоречие получаем из-за оговорки в общем случае, что $\exists C: P_A\left(C\right), \ P_B\left(C\right) > 0$, то есть что распределения пересекаются, если не пересекаются, то никаких противоречий)
\end{proof}
\item \textbf{Сильная состоятельность}. $\lim_{n \to +\infty} \widehat{\theta}_n\left(X_1, \cdots, X_n\right) = \theta$ почти наверное на распределении $F_\theta$
\item \textbf{Асимптотическая нормальность}. Оценка называется асимптотически нормальной, если 
\[
    \sqrt{n}\left(\widehat{\theta}_n\left(X_1, \cdots, X_n\right) - \theta\right) \xrightarrow{d} N\left(0, \sigma^ 2\left(\theta\right)\right)
\]
Это условие влечет состоятельность и позволяет оценивать вероятности событий \\ $\alpha < \widehat{\theta}_n\left(X_1, \cdots, X_n\right) < \beta$ через нормальные распределения.
\begin{advice}
Если оценка асимптотически номальная, то она состоятельна.
\end{advice}
\begin{proof}
 $\widehat{\theta}_n\left(X_1, \cdots, X_n\right) - \theta \xrightarrow{d} 0$ (отсылка к первой теореме), $\theta \xrightarrow{P} \theta$ \\ $\Rightarrow$ $\widehat{\theta}_n\left(X_1, \cdots, X_n\right) \xrightarrow{P} \theta$ (взяли сумму первых двух)
\end{proof}
\end{enumerate}
\begin{advice}
Если эффективная оценка существует, то она единственна.
\end{advice}
\begin{proof}
Пусть $\theta^*_1$ и $\theta^*_2$ две эффективные оценки. Тогда оценка $\frac{\theta^*_1 + \theta^*_2}{2}$ также является несмещенной. Кроме того (пусть $\mathbb{D} \theta^*_1 = \mathbb{D} \theta^*_2 = a ^ 2$),
\[
    \mathbb{D}\frac{\theta^*_1 + \theta^*_2}{2} = \frac 1 4 a ^ 2 + \frac 1 2 \text{cov}\left(\theta^*_1, \theta^*_2\right) + \frac 1 4 a ^ 2 \geq a ^ 2
\]
то есть $\text{cov}\left(\theta^*_1, \theta^*_2\right) \geq a ^ 2$. Тогда получим
\[
    0 \leq \mathbb{D}\left[\theta^*_1 - \theta^*_2\right] = 2 a ^ 2 - 2 \text{cov}\left(\theta^*_1, \theta^*_2\right) \leq 0
\]
Иначе говоря, $\theta^*_1 = \theta^*_2$ п.н.
\end{proof}
\section{Метод моментов}
У нас есть некоторая простая выборка $\left(x_1, \cdots, x_n\right)$, взятая из распределения $F_\theta$, мы хотим по ней получить состоятельную оценку для $\theta$.
\par
Пусть $g \ -$ непрерывная функция (методом моментов он называется, потому что обычно в качестве $g$ берется степенная функция), что $\mathbb{E}_\theta \abs{g\left(X\right)} < \infty$. Тогда введем функцию $f\left(\theta\right) := \mathbb{E}_\theta g\left(X\right)$. Матожидание является функцией от $\theta$, потому что неизвестно нам только распределение, которое зависит от $\theta$. Теперь допустим, что $\exists f^{-1}$ на $\Theta$. Тогда легко найти $\theta$:
\[
    \theta = f ^ {-1}\left(f\left(\theta\right)\right) = f ^ {-1}\left(\mathbb{E}_\theta g\left(X\right)\right)
\]
\par Но так как распределения мы не знаем, у нас есть только простая выборка из него, то поступаем так. Знаем, что по ЗБЧ
\[
    \frac{g\left(X_1\right) + \cdots + g\left(X_n\right)}{n} \xrightarrow{p} \mathbb{E}_{\theta}g\left(X\right) = f\left(\theta\right)
\]
Тогда по теоремам непрерывности (предложение 1):
\[
    f ^ {-1}\left(\frac{g\left(X_1\right) + \cdots + g\left(X_n\right)}{n}\right) \xrightarrow{p} \theta
\]
Тогда
\[\widehat{\theta}_n\left(X_1, \cdots, X_n\right) = f ^ {-1}\left(\frac{g\left(x_1\right) + \cdots + g\left(x_n\right)}{n}\right)\]
\begin{enumerate}
\item Оценка получилась состоятельная.
\item Покажем асимптотическую нормальность оценки. По ЦПТ знаем, что
\[
    \xi_n = \sqrt{n}\left(\frac{g\left(X_1\right) + \cdots + g\left(X_n\right)}{n} - \mathbb{E}_\theta g\left(X\right)\right) \xrightarrow{d} \xi \sim N\left(0, \mathbb{D}_\theta g\left(X\right)\right)
\]
Обозначим 
\[
    a = \mathbb{E}_\theta g\left(X\right), \ \sigma ^ 2 = \mathbb{D}_\theta g\left(X\right)
\]
Тогда, предполагая, что $f ^ {-1}$ непрерывно дифференцируема, по теореме непрерывности (предложение 6), получаем:
\[
    \sqrt{n}\left(\widehat{\theta}_n\left(X_1, \cdots, X_n\right) - \theta\right) = \sqrt{n} \left(f ^ {-1}\left(a + \frac{\sigma}{\sqrt{n}} \xi_n\right) - f ^ {-1}\left(a\right)\right) \xrightarrow{d} \xi\left(f ^ {-1}\right)'\left(a\right) 
\]
 То есть, если $\left(f ^ {-1}\right)'\left(a\right)  \neq 0$, то оценка $\widehat{\theta}_n\left(X_1, \cdots, X_n\right)$ асимптотически нормальна с коэффициентом $\sigma ^ 2\left(\theta\right) = \left(\left(f ^ {-1}\right)'\left(a\right)\right) ^ 2 \sigma ^ 2$
 \end{enumerate}
 \begin{example}
 Возьмем выборку $\left(X_1, \cdots, X_n\right)$ из равномерного распределения $U_{\left[0, \theta\right]}$, где $\theta > 0$. Найдем оценку по методу моментов при $f\left(x\right) = x$:
 \[
    \mathbb{E}_\theta f\left(X\right) = \frac{\theta}{2} \Rightarrow \theta = 2 \mathbb{E}_\theta f\left(X\right)
 \]
Получаем оценку
\[
    \widehat{\theta}_n = 2 \overline{X}
 \]
 \end{example}
\section{Информация Фишера и неравенство Рао-Крамера}
\begin{definition}
\textit{Несмещенной оценкой нуля} называется такая статистика, $U\left(X_1, \cdots, X_n\right)$, для которой верно $\mathbb{E}_\theta U = 0$, $\forall \theta$ (можно опять провести аналогии со скалярным произведением)
\end{definition}
\begin{statement}
$\widehat{\theta} \ - $ оптимальная (минимальная по дисперсии) $\Leftrightarrow$
$\mathbb{E}\left[\widehat{\theta} U\right] = 0$ $\forall U \ - $ несмещеннных оценок нуля.
\end{statement}
\begin{proof}
$\Rightarrow$
\\
Заметим, что $\forall \lambda$ верно $\mathbb{E}\left(\widehat{\theta} + \lambda U\right) = \widehat{\theta}$ и из оптимальности $\mathbb{D} \widehat{\theta} \leq \mathbb{D}\left(\widehat{\theta} + \lambda U\right)$. Раскрыв дисперсию, получим
\[
    \mathbb{D} \widehat{\theta} \leq \mathbb{D} \widehat{\theta} + \lambda ^ 2 \mathbb{D} U + 2 \lambda\text{cov}\left(\widehat{\theta}, U\right)
\]\[
    2\lambda\text{cov}\left(\widehat{\theta}, U\right) + \lambda ^ 2 \mathbb{D} U \geq 0
\]
Это верно только, когда $\text{cov}\left(\widehat{\theta}, U\right) = 0$ (потому что иначе есть $\lambda$, в которых неравенство не выполняется). Осталось записать
\[
    \text{cov}\left(\widehat{\theta}, U\right) = \mathbb{E} \left[\left(\widehat{\theta} - \theta\right) U\right] = \mathbb{E} \left[\widehat{\theta} U\right] = 0
\]
$\Leftarrow$ \\
Возьмем другую несмещенную оценку $\tilde{\theta}$. Рассмотрим ее дисперсию:
\[
    \mathbb{D} \tilde{\theta} = \mathbb{E}\left(\tilde{\theta} - \theta\right) ^ 2 = \mathbb{E} \left(\left(\tilde{\theta} - \widehat{\theta}\right) + \left(\widehat{\theta} - \theta\right)\right) ^ 2 = \mathbb{E}\left(\tilde{\theta} - \widehat{\theta}\right)^  2 + 2 \mathbb{E} \left[\left(\tilde{\theta} - \widehat{\theta}\right)\left(\widehat{\theta} - \theta\right)\right] + \mathbb{E}\left(\widehat{\theta} - \theta\right) ^ 2
\]
Заметим, что 
\[
    \mathbb{E} \left[\left(\tilde{\theta} - \widehat{\theta}\right)\left(\widehat{\theta} - \theta\right)\right] = \mathbb{E} \left[\left(\tilde{\theta} - \widehat{\theta}\right) \widehat{\theta}\right] = 0
\]
Последнее равенство вытекает, из условия, если взять $U = \tilde{\theta} - \widehat{\theta}$. Тогда получаем, что 
\[
    \mathbb{D}\tilde{\theta} = \mathbb{D} \widehat{\theta} + \mathbb{D} \left(\tilde{\theta} - \widehat{\theta}\right) \geq \mathbb{D} \widehat{\theta}
\]
То есть $\widehat{\theta}$ оптимальна.

\end{proof}

\begin{example}
Пусть $X \sim Bern\left(\theta\right)$. Рассмотрим оценку $\overline{X}$. Она несмещенная и по ЗБЧ состоятельная.  Докажем оптимальность. По доказанному утверждению, нужно убедиться, что $\mathbb{E} \overline{X} U = 0$ $\forall U \ -$ несмещенных оценок нуля. Запишем $\forall \theta$
\[
    0 =\mathbb{E} U\left(X_1, \cdots, X_n\right) = \sum_{\varepsilon_i \in \left\{0, 1\right\} } U\left(\varepsilon_1, \cdots, \varepsilon_n\right) \theta ^ {\sum \varepsilon_i} \left(1 - \theta\right) ^ {n - \sum \varepsilon_i} = 
\]
Сгруппируем
\[
    = \sum_{k = 0}^{n}  \left[\theta ^ {k} \left(1 - \theta\right) ^ {n - k} \sum_{\varepsilon_i: \sum \varepsilon = k} U\left(\varepsilon_1, \cdots, \varepsilon_n\right)\right] = \sum_{k = 0}^{n}  \theta ^ {k} \left(1 - \theta\right) ^ {n - k} Q_k = \frac 1 {\theta ^ n} \sum_{k = 0}^{n}  \left(\frac{\theta}{1 - \theta}\right) ^ k Q_k = 0
\]
При $\theta \in \left(0, 1\right)$, $\frac{\theta}{1 - \theta} \in \left(0, +\infty\right) \ - $ на этом промежутке многочлен равен нулю, тогда получается, что $Q_k = 0$. Теперь можем проверить $\overline{X}$ на оптимальность:
\[
    \mathbb{E} \overline{X} U = \sum_{k = 0}^{n} \left[\frac{k}{n} \sum_{\varepsilon_i: \sum \varepsilon = k} U\left(\varepsilon_1, \cdots, \varepsilon_n\right) \theta ^ {k} \left(1 - \theta\right) ^ {n - k}\right] = \sum_{k = 0}^{n} \frac k n Q_k \theta ^ {k} \left(1 - \theta\right) ^ {n - k} = 0
\]
Получили оптимальность $\overline{X}$. 
\end{example}
В методе максимального правдоподобия мы более подробно разберем понятие информации, а пока обойдемся сухими определениями.
\begin{definition}
Пусть $X = \left(X_1, \cdots, X_n\right)$ простая выборка из распределения с плотностью $\rho_\theta$, тогда функция $p\left(x, \theta\right) = \rho_{\theta}\left(x_1\right) \cdots \rho_\theta\left(x_n\right)$ называется \textit{функцией правдоподобия}, однако с произведением работать неудобно, поэтому прологарифмируем: $L\left(x, \theta\right) = \sum_{j = 1}^n \ln \rho_\theta\left(x_j\right)$
\end{definition}
\begin{definition}
Величина $I\left(\theta\right) = \mathbb{E}_\theta \left[\frac{\partial L}{\partial \theta}\right] ^ 2$ называется \textit{информацией Фишера}.
\end{definition}
В дальнейших рассуждениях предполагается выполнение условий регулярности для функции правдоподобия, что она непрерывна и дифференцируема по $\theta$ и что операции дифференцирования и интегрирования перестановочны ($\int f' dx = \left(\int f dx\right)'$)
\begin{advice}(аддитивность информации Фишера) Верно равенство
\[
    I\left(\theta\right) = n i\left(\theta\right),
\]
где $i\left(\theta\right) \ - $ информация Фишера для выборки из одного элемента.
\end{advice}
\begin{proof}
Заметим, что 
\[
    \mathbb{E} \frac{\partial L}{\partial \theta} = \mathbb{E}\left[\sum_{j = 1}^{n} \frac{\rho'_\theta\left(x_j\right)}{\rho_\theta\left(x_j\right)}\right] = \sum_{j =1}^{n} \int \frac{\rho'_\theta\left(x_j\right)}{\rho_\theta\left(x_j\right)} \rho_\theta\left(x_j\right) dx = 
    \sum_{j = 1}^{n} \left(\int_{-\infty}^{+\infty} \rho_\theta\left(x_j\right) dx\right)' = \sum_{j = 1}^{n} \left(1\right)' = 0
\]
Поэтому 
\[
    I\left(\theta\right) = \mathbb{E} \left[\frac{\partial L}{\partial \theta}\right] ^ 2 = \mathbb{D} \left[\frac{\partial L}{\partial \theta}\right] = \mathbb{D} \left[\sum_{j = 1}^{n} \frac{\rho'_\theta\left(x_j\right)}{\rho_\theta\left(x_j\right)}\right] = \sum_{j = 1}^n \mathbb{D} \left[\frac{\rho'_\theta\left(x_j\right)}{\rho_\theta\left(x_j\right)}\right] = \sum_{j = 1}^n i\left(\theta\right) = ni\left(\theta\right)
\]
\end{proof}

Рассмотрим неравенство, позволяющее оценивать оптимальные оценки.
\begin{theorem} (неравенство Рао-Крамера) Выполняются условия регулярности и $I\left(\theta\right) > 0$ (если $= 0$, то это значит, что мы от эксперимента не получаем никакой информации и все бессмысленно). Пусть $\theta_n\left(X\right) \ - $ произвольная несмещенная оценка статистики $\tau\left(\theta\right)$. Тогда верно
\[
    \mathbb{D} \theta_n\left(X\right) \geq \frac{\left(\tau'\left(\theta\right)\right) ^ 2}{I\left(\theta\right)}
\]
\end{theorem}
\begin{proof}
Из несмещенности знаем 
\[
    \mathbb{E} \theta_n\left(X\right) = \int \theta_n\left(x_1, \cdots, x_n\right) p\left(x_1, \cdots, x_n, \theta\right) dx_1 \cdots d x_n = \tau\left(\theta\right) = A
\]
Также знаем
\[
    \int p\left(x_1, \cdots, x_n, \theta\right) dx_1 \cdots d x_n = 1 = B
\]
Рассмотрим выражение $A'_\theta - \theta B'_\theta$ ($= \tau'\left(\theta\right)$ по понятным причинам):
\[
    \tau'\left(\theta\right) = \int \left(\theta_n\left(x\right) - \theta\right) p'\left(x, \theta\right) dx = \int \left(\theta_n\left(x\right) - \theta\right) \frac{p'\left(x, \theta\right)}{p\left(x, \theta\right)} p\left(x, \theta\right) dx = \mathbb{E} \left[\left(\theta_n - \theta\right) \frac{\partial L}{\partial \theta}\right]
\]
Из неравенства Коши-Буняковского получим: 
\[
    \tau'\left(\theta\right) = \mathbb{E} \left[\left(\theta_n - \theta\right) \frac{\partial L}{\partial \theta}\right] \leq \sqrt{\mathbb{D} \theta_n} \sqrt{\mathbb{E} \left[\frac{\partial L}{\partial \theta}\right] ^ 2}
\]\[
    \left(\tau'\left(\theta\right)\right) ^ 2 \leq I\left(\theta\right)\mathbb{D} \theta_n 
\]
\end{proof}

\begin{remark}
Можно переписать неравенство в виде
\[
    \mathbb{D} \theta_n \leq \frac{\left(\tau'\left(\theta\right)\right) ^ 2}{n i\left(\theta\right)}
\]
То есть как бы мы не старались, точность оценки будет порядка $\frac 1 n$.
\end{remark}

\clearpage
\section{Метод максимального правдоподобия}
Во-первых рассмотрим пример, объясняющий понятие информации.
\begin{example}
Пусть есть монетка $M$ с $P\left(M = 0\right) = q$, $P\left(M = 1\right) = p$. Подбрасывается монетка, и, в зависимости от исхода, генерируется случайное событие из распределение $\rho_0$ или $\rho_1$. Таким образом, получили случайную величину $\xi$. Изначально шансы, что случайная величина была взята из первого распределения против нулевого $\frac p q$. Несложно вычилисть шансы после одной генерации случайного события. Воспользовавшись аналогом формулы Байеса, получим
\[
    P\left(M = 1 | \xi = x\right) = \frac{\rho_1\left(x\right)p}{\rho_1\left(x\right)p + \rho_0\left(x\right)q}, \; P\left(M = 0 | \xi = x\right) = \frac{\rho_0\left(x\right)q}{\rho_1\left(x\right)p + \rho_0\left(x\right)q}
\] 
То есть теперь шансы $\frac{\rho_1\left(x\right) p}{\rho_0\left(x\right) q}$. Они изменились. Пусть \textit{информация} $-$ это то, как изменились шансы. Как можно измерить информацию, полученную в ходе эксперимента? Например, возьмем разницу $\ln$ от шансов:
\[
    \ln \frac{\rho_1\left(x\right) p}{\rho_0\left(x\right) q} - \ln \frac p q= \ln \frac{\rho_1\left(x\right)}{\rho_0\left(x\right)}
\]
Ну и чтобы для каждой величины не вычислять выражение, усредним по распределению $\rho_1$ (можно было и по $\rho_0$ усреднять) и будем называть это выражение информацией: 
\[
    \tilde{I} = \int \ln \frac{\rho_1\left(x\right)}{\rho_0\left(x\right)} \rho_1\left(x\right) dx = \int \ln \rho_1\left(x\right) \rho_1\left(x\right) dx - \int \ln \rho_0\left(x\right) \rho_1\left(x\right) dx
\]
\end{example}
\begin{statement} (неравенство информации)
Пусть $\rho_0, \rho_1 \ - $ вероятностные плотности. Тогда 
\[
    \int \ln \rho_1\left(x\right) \rho_1\left(x\right) dx \geq \int \ln \rho_0\left(x\right) \rho_1\left(x\right) dx,
\]
то есть $\tilde{I} \geq 0$. Причем равенство выполняется $\Leftrightarrow$ $\rho_0 \equiv \rho_1$
\end{statement}
\begin{proof}
Хотим доказать
\[
    \int \ln \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} \rho_1\left(x\right) dx \leq 0 
\]
Знаем, что $\ln x \leq x - 1$. Тогда 
\[
    \ln \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} \rho_1\left(x\right) \leq \rho_0\left(x\right) - \rho_1\left(x\right)
\]
Подставляя в интеграл, получим, что 
\[
    \int \ln \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} \rho_1\left(x\right) dx \leq 1 - 1 = 0
\]
Теперь посмотрим, когда выполняется равенство. Оно выполнятется, когда $\ln x = x - 1$, то есть 
\[
    \int \left(\frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} - 1 + \ln \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)}\right) \rho_1\left(x\right) dx = 0
\]
Тогда 
\[
    \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} - 1 + \ln \frac{\rho_0\left(x\right)}{\rho_1\left(x\right)} = 0
\]
почти наверное (так как это выражение $\geq 0$), то есть $\rho_0 = \rho_1$ почти наверное.
\end{proof}
То есть, если $\tilde{I} = 0$, то от новой информации ничего не меняется, она бессмысленна. Также добавим, что $\tilde{I}$ в реальности оценивает расстояние между распределениями, это называется энтропией распределения $\rho_1$ относительно $\rho_0$.
\par 
Применим полученные знания для построения оценки. Пусть $X = \left(X_1, \cdots, X_n\right) \ - $ выборка из распределения с параметром $\theta = \theta_1$. Рассмотрим функцию $W\left(\theta\right) = \mathbb{E}_{\theta_1} \ln \rho_\theta\left(X_1\right)$, ее называют истинным правдоподобием. Заметим из неравенства информации, что максимум истинного правдоподобия достигается только на $\theta = \theta_1$. Само матожидание вычислять мы не умеем, но по ЗБЧ значем, что 
\[
    \frac 1 n L\left(X, \theta\right) = \frac 1 n \sum \ln \rho_\theta\left(x_j\right) \xrightarrow{P} W\left(\theta\right)
\]
Тогда \textit{оценкой максимального правдоподобия} называется оценка $\widehat{\theta} = \text{argmax}_\theta L\left(X, \theta\right)$. Если посмотреть, что мы максимизируем функцию $L$, и вспомнить, что это по сути функция правдоподобия, то выходит, что мы максимизируем вероятность получения выборки $X$ из распределения с параметром $\widehat{\theta}$. Еще заметим, что $W\left(\theta_1\right) - W\left(\theta\right)$ это величина из примера с монетой, то есть количество информации, которое дало одно наблюдение.

\begin{statement} (состоятельность оценки методом максимального правдоподобия) Дана выборка $X = \left(X_1, \cdots, X_n\right)$. Пусть $\theta \in \left(\alpha, \beta\right)$ и на этом промежутеке у функции $L\left(X_1, \cdots, X_n, \theta\right)$ $\exists! \widehat{\theta}_n \ - $ точка локального максимума, тогда $\widehat{\theta}_n \xrightarrow{P} \theta_1$, то есть оценка состоятельна.

\end{statement}
\begin{proof}
По ЗБЧ $\frac 1 n L\left(X, \theta\right) \xrightarrow{P} W\left(\theta\right)$. Хотим $\forall \delta > 0$: $\lim_{n \to \infty} P\left(\abs{\widehat{\theta}_n - \theta} \geq \delta\right) = 0$. Из информационного неравенства заметим, что $W\left(\theta_1 - \delta\right) < W\left(\theta_1\right) > W\left(\theta_1 + \delta\right)$. Знаем, что $\frac 1 n L\left(X, \theta_1 - \delta\right), $ \\ $ \frac 1 n L\left(X, \theta_1\right), \ \frac 1 n L\left(X, \theta_1 + \delta\right)$ сходятся по вероятности соответственно к $W\left(\theta_1 - \delta\right), \ W\left(\theta_1\right), \ W\left(\theta_1 + \delta\right)$. Докажем, что сохраняются те же знаки неравенства. 
\par Пусть $\varepsilon = W\left(\theta_1\right) - W\left(\theta_1 - \delta\right) > 0$. Знаем, что 
\[
    P\left(\abs{\frac 1 n L\left(X, \theta_1 - \delta\right) - W\left(\theta_1 - \delta\right)} \geq \frac \varepsilon {10}\right) \xrightarrow{n \to \infty} 0
\]\[
    P\left(\abs{\frac 1 n L\left(X, \theta_1\right) - W\left(\theta_1\right)} \geq \frac {\varepsilon }{10}\right) \xrightarrow{n \to \infty} 0
    
\]
Тогда 
\[
    P\left(\frac 1 n L\left(X, \theta_1 - \delta\right) \geq \frac 1 n L\left(X, \theta_1\right)\right) \leq \]\[ \leq P\left(\left\{\abs{\frac 1 n L\left(X, \theta_1 - \delta\right) - W\left(\theta_1 - \delta\right)} \geq \frac \varepsilon {10}\right\} \bigcup \left\{\abs{\frac 1 n L\left(X, \theta_1\right) - W\left(\theta_1\right)} \geq \frac \varepsilon {10}\right\}\right) \leq
\]\[
    \leq P\left(\abs{\frac 1 n L\left(X, \theta_1 - \delta\right) - W\left(\theta_1 - \delta\right)} \geq \frac \varepsilon {10}\right) + P\left(\abs{\frac 1 n L\left(X, \theta_1\right) - W\left(\theta_1\right)} \geq \frac {\varepsilon }{10}\right) \xrightarrow{n \to \infty} 0 
\]
Допустим противное, что в первом неравенстве может стоять $< \frac \varepsilon {10}$ в обоих случаях. Тогда выходит
\[
    \varepsilon = W\left(\theta_1\right) - W\left(\theta_1 - \delta\right) \leq
    \frac 1 n L\left(X, \theta_1\right) + \frac \varepsilon {10} - W\left(\theta_1 - \delta\right) \leq \frac 1 n L\left(X, \theta_1\right) + \frac \varepsilon {10} - \frac 1 n L\left(X, \theta_1 - \delta\right) + \frac \varepsilon {10} \leq \frac \varepsilon 5
\]
Получили противоречие, то есть все ок. То есть вышло, что $\frac 1 n L\left(X, \theta_1 - \delta\right) < \frac 1 n L\left(X, \theta_1\right)$ почти наверное. Второе неравенство доказывается аналогично.
\par Получив желаемые неравенства, делаем вывод, что внутри отрезка $\left[\theta_1 - \delta, \theta_1 + \delta\right]$ есть локальный максимум, то есть $\abs{\widehat{\theta}_n - \theta_1} < \delta$ (строгое неравенство, потому что мы доказывали строгие неравенства для $L$)

\end{proof}

\begin{remark}
Теперь обсудим, откуда возникает информация Фишера. У нас есть функция истинного правдоподобия $W\left(\theta\right)$. Разложим ее по Тейлору в точке $\theta_1$ (предполагаем, что зависимость он $\theta$ позволяет дифференцировать под знаком интеграла). $W'\left(\theta\right) = \int \frac{\rho_\theta'\left(x\right)}{\rho_\theta\left(x\right)} \rho_{\theta_1}\left(x\right) dx$, соответственно $W'\left(\theta_1\right) = 0$. $W''\left(\theta\right) = \int \frac{\rho_\theta''\left(x\right)}{\rho_\theta\left(x\right)} \rho_{\theta_1}\left(x\right) dx - \int \left(\frac{\rho_\theta'\left(x\right)}{\rho_\theta\left(x\right)}\right) ^ 2 \rho_{\theta_1}\left(x\right) dx$, соответственно $W''\left(\theta_1\right) = 0 - \int \left(\frac{\partial \ln \rho_\theta\left(x\right)}{\partial \theta}\right) ^ 2 \rho_{\theta_1}\left(x\right) dx = -I\left(\theta_1\right)$. В итоге получаем: \[
    W\left(\theta\right) \approx W\left(\theta_1\right) - \frac 1 2 I\left(\theta_1\right) \left(\theta - \theta_1\right) ^ 2
\]
То есть она выражает скорость изменения информации в окрестности максимума $\theta_1$.

\end{remark}
\begin{remark}
Теперь вспомним неравенство Рао-Крамера. Равенство в неравенстве Коши-Буняковского выполняется только в случае пропорциональности $\theta_n\left(X\right) - \tau\left(\theta\right)$ и $\frac{\partial L}{\partial \theta}$. То есть, когда 
\[
    \theta_n\left(X\right) - \tau\left(\theta\right) = C\left(\theta\right)\frac{\partial L}{\partial \theta}
\]
Это знание в некоторых случаях помогает находить эффективные оценки
\end{remark}
\begin{remark}
Также отметим, что если существует несмещенная оценка $\theta_n\left(X\right)$ параметра $\theta$, на которой достигается равенство в Рао-Крамере, то это обязательно оценка максимального правдоподобия. Действительно, верно равенство 
\[
    \theta_n\left(X\right) - \tau\left(\theta\right) = C\left(\theta\right)\frac{\partial L}{\partial \theta}
\]
Подставив вместо $\theta$ оценку максимального правдоподобия, получим
\[
    \theta_n\left(X\right) - \widehat{\theta}_n\left(X\right) = 0
\]
То есть они равны.
\end{remark}
\begin{remark}
И, наконец, сделаем небольшое замечание о том, откуда берется асимптотическая нормальность оценки максимального правдоподобия. Разложим $\frac{\partial L}{\partial \theta}\left(X, \theta\right)$ по Тейлору в $\theta_1$ (примерно, без о малых):
\[
    \frac{\partial L}{\partial \theta}\left(X, \theta\right) \approx \frac{\partial L}{\partial \theta}\left(X, \theta_1\right) + \frac{\partial ^ 2 L}{\partial \theta ^ 2}\left(X, \theta_1\right)\left(\theta - \theta_1\right)
\]
Если мы подставим $\theta = \widehat{\theta}_n\left(X\right) \ - $ оценку максимального правдоподобия, то $\frac{\partial L}{\partial \theta}\left(X, \widehat{\theta}_n\left(X\right)\right) = 0$ просто из определения оценки максимального правдоподобия. Тогда получаем:
\[
    \widehat{\theta}_n - \theta_1 = -\frac{\frac{\partial L}{\partial \theta}}{\frac{\partial ^ 2 L}{\partial \theta ^ 2}}
\]
Домножим на $\sqrt{n}$, чтобы стало как в ЦПТ. Берем $\frac 1 n \frac{\partial L}{\partial \theta} = \frac 1 n \sum_{j = 1}^{n} \frac{\partial}{\partial \theta} \ln \rho_{\theta_1}\left(x_j\right)$. Помним, что \\ $\mathbb{E} \frac{\partial}{\partial \theta} \ln \rho_{\theta_1}\left(x_j\right) = 0 = \mu$. Теперь дисперсия: 
\[\sigma ^ 2 = n\mathbb{D} \left[\frac 1 n \frac{\partial}{\partial \theta} \ln \rho_{\theta_1}\left(x_j\right)\right] = \frac 1 n \mathbb{E} \left[\frac{\partial}{\partial \theta} \ln \rho_{\theta_1}\left(x_j\right)\right] ^ 2 = i\left(\theta_1\right)
\]
Тогда из ЦПТ получаем
\[
    \sqrt{n} \left(\frac 1 n\frac{\partial L}{\partial \theta} - 0\right) \xrightarrow{d} N\left(0, i\left(\theta_1\right)\right)
\]
Теперь разберемся с 
\[
    \frac 1 n \frac{\partial ^ 2 L}{\partial \theta ^ 2} = \frac 1 n \sum_{j = 1}^{n} \frac{\partial ^ 2}{\partial \theta ^ 2} \ln \rho_{\theta_1}\left(x_j\right) \xrightarrow{P} -i\left(\theta_1\right)
\]
В итоге получаем, что $\sqrt{n}\left(\widehat{\theta}_n - \theta_1\right) \xrightarrow{d} N\left(0, \frac 1 {i\left(\theta_1\right)}\right)$
\end{remark}

\clearpage

\section{Доверительные интервалы}

Ясно, что информации о состоятельности оценки $\widehat{\theta}_n\left(X\right)$ не достаточно для того, чтобы что-то говорить о возможном значении $\theta$. Предположим, что мы знаем ``скорость сходимости'', то есть для фиксированного $\alpha \in \left(0, 1\right)$, для фиксированного $\varepsilon > 0$ мы можем подобрать такой номер $n$, начиная с которого $P_\theta\left(\abs{\widehat{\theta}_n\left(X\right) - \theta} < \varepsilon\right) > 1 - \alpha$. Тогда параметр $\theta$ с большой вероятностью $\theta \in \left(\widehat{\theta}_n\left(X\right) - \varepsilon, \widehat{\theta}_n\left(X\right) + \varepsilon\right)$.
\begin{definition}
Обощая данное наблюдение, приходим к следующей конструкции. Пусть заданы две статистики $\theta_1\left(X\right)$ и $\theta_2\left(X\right)$. Случайный интервал $\left(\theta_1\left(X\right), \theta_2\left(X\right)\right)$ называется $\textit{доверительным интервалом}$ с уровнем доверия $1 - \alpha$, если при всех $\theta$ верно
\[
    P\left(\theta_1\left(X\right) < \theta < \theta_2\left(X\right)\right) \geq 1 - \alpha
\]
Если же задана последовательность пар статистик $\theta_1^n\left(X\right)$ и $\theta_2^n\left(X\right)$, для которой 
\[
    \lim_{n \to \infty} \text{inf} P\left(\theta_1^n\left(X\right) < \theta < \theta_2^n\left(X\right)\right) \geq 1 - \alpha
\]
то говорят, что задан \textit{асимптотический доверительный интервал} уровня доверия $1 - \alpha$.
\end{definition}

\begin{example}
Пусть задана выборка $X_1, \cdots, X_n$ из распределения $N\left(\theta, 1\right)$. Рассмотрим статистику $\overline{X}_n$, тогда $\sqrt{n}\left(\overline{X}_n - \theta\right) \sim N\left(0, 1\right)$. Пусть число $z_\gamma$ выбрано так, что $\Phi\left(z_\gamma\right) = \gamma, \ -$ квантиль нормального распределения. Тогда
\[
    P_\theta\left(z_{\alpha/2} < \sqrt{n}\left(\overline{X}_n - \theta\right) < z_{1-\alpha/2}\right) = 1 - \alpha
\]
Так как $z_{\alpha/2} = -z_{1-\alpha/2}$, то перепишем
\[
    P_\theta\left(\overline{X}_n - \frac{z_{1-\alpha/2}}{\sqrt{n}} < \theta < \overline{X}_n + \frac{z_{1-\alpha/2}}{\sqrt{n}}\right) = 1 - \alpha
\]
Получили доверительный интервал. Заметим, что так как он симметричный, то он минимальный по длине среди всех доверительных интервалов такого же уровня доверия (надо было найти в стандартном распределении интервал, содержащий вероятнось $1 - \alpha$, смотрим на график плотности и все понимаем).
\end{example}

\par
Для постороения доверительных интервалов бывает полезным использовать неравенство Чебышева или Чернова (Хефдинга-Чернова). 
\begin{example}
Пусть $X_1, \cdots, X_n$ выборка из распределения Бернулли с вероятностью успеха $\theta$. По неравенству Хефдинга-Чернова
\[
    P\left(\abs{\overline{X}_n - \theta}\geq t\right) \leq 2e ^ {-\frac{nt^2}{4}}
\]
Взяв $t$ так, чтобы $e ^ {-\frac{nt^2}{4}}= \frac{\alpha}2$, получаем доверительный интервал \[\left(\overline{X}_n - \frac{2\sqrt{-\ln\left(\alpha / 2\right)}}{\sqrt{n}}, \overline{X}_n + \frac{2\sqrt{-\ln\left(\alpha / 2\right)}}{\sqrt{n}}\right)
\]

\end{example}
\par 
Рассмотрим общий метод построения доверительных интервалов с помощью центральных статистик.
\begin{definition}
Статистика $V\left(X, \theta\right)$ называется $\textit{центральной}$, если ее распределение не зависит от $\theta$ и при фиксированном $X$ эта функция монотонна (по $\theta$).
\end{definition}
\par 
Тогда в силу независимости распределения от параметра $\theta$, подберем такие $v_1$, $v_2$, что 
\[
    P_\theta\left(v_1 < V\left(X, \theta\right) < v_2\right) = 1 - \alpha
\]
В силу монотонности (берем обратную функцию с сохранением всех неравенств), получаем доверительный интервал
\[
    P_\theta\left(V^{-1}\left(v_1\right) < \theta < V^{-1}\left(v_2\right)\right) = 1 - \alpha
\]
\begin{example}
Пусть $X_1, \cdots, X_n \ -$ выборка из $U\left[0, \theta\right]$. Тогда $\theta^{-1} X_j \sim U\left[0, 1\right]$. Рассмотрим статистику $V\left(X, \theta\right) = \theta ^ {-1} X_{\left(n\right)}$, тогда $F_{\theta ^ {-1} X_{\left(n\right)}}\left(t\right) = t ^ {n}$ при $t \in \left[0, 1\right]$ и 
\[
    1 - \alpha = P\left(\alpha ^ {1/n} < V\left(X, \theta\right) < 1\right) = P\left(X_{\left(n\right)} < \theta< \alpha^{-1/n} X_{\left(n\right)}  \right)
\]
Заметим, что длина доверительного интервала пропорциональна $\alpha ^ {-1/n} - 1 = O\left(\frac 1 n \right)$, то есть с ростом размера выборки, точность интервала также увеличивается.
\end{example}
\par В качестве $V$ бывает полезным взять $\sum_{j=1}^n \ln F_{\theta}\left(X_j\right)$.
\begin{example}
Рассмотрим типичный пример \textbf{посторения асимптотического доверительного интервала}. Пусть $\theta_n\left(X\right)$ асимптотически нормальная оценка параметра $\theta$ с асимптотической дисперсией $\sigma^ 2\left(\theta\right)$, то есть 
\[
    \sqrt{n} \frac{\theta_n\left(X\right) - \theta}{\sigma\left(\theta\right)} \xrightarrow{d} N\left(0, 1\right)
\]
Тогда аналогично 19 примеру, подбираем границы $z_{1-\alpha/2}$ и $z_{\alpha/2}$ (напомню, что $\Phi\left(z_{1-\alpha/2}\right) = 1 - \alpha/2$)
\[
    P\left(z_{\alpha/2} < \sqrt{n} \frac{\theta_n\left(X\right) - \theta}{\sigma\left(\theta\right)} < z_{1-\alpha/2}\right) \xrightarrow{n \to \infty} 1 - \alpha
\]\[
    P\left(\theta_n\left(X\right) - \frac{\sigma\left(\theta\right)z_{1-\alpha/2}}{\sqrt{n}} < \theta < \theta_n\left(X\right) + \frac{\sigma\left(\theta\right)z_{1-\alpha/2}}{\sqrt{n}}\right)\xrightarrow{n \to \infty} 1 - \alpha
\]

Для получения доверительного интервала осталось только избавиться от $\sigma{\left(\theta\right)}$. Понятно, что ее можно заменить любой состоятельной оценкой (чтобы оставалась сходимость по распределению к $N\left(0, 1\right)$), например, выборочной дисперсией или $\sigma\left(\theta_n\left(X\right)\right)$.
\end{example}

\par 
Рассмотрим теперь подробнее построение доверительных интервалов для \textbf{параметров $\mu$ и $\sigma$ нормального распределения $N\left(\mu, \sigma ^ 2\right)$}.

\par
\textbf{$\sigma $ известна, оцениваем $\mu$}. $\sigma$ может быть известна, например, если при замерах мы пользуемся прибором с известной точностью. Пусть $X_1, \cdots, X_n \ - $ выборка из $N\left(\mu, \sigma ^ 2\right)$. Тогда 
\[
    \frac{\sqrt{n}\left(\overline{X}_n - \mu\right)}{\sigma} \sim N\left(0, 1\right)
\]
Аналогично прошлому примеру, получаем доверительный интервал уровня доверия $1 - \alpha$ вида
\[
    \left(\overline{X}_n - \frac{\sigma z_{1-\alpha/2}}{\sqrt{n}}, \overline{X}_n + \frac{\sigma z_{1-\alpha/2}}{\sqrt{n}}\right)
\]
\par
\textbf{$\sigma$ неизвестна, оцениваем $\mu$}. Пусть $X_1, \cdots, X_n \ -$ выборка из $N\left(\mu, \sigma\right)$. Мы знаем (см. пример 10), что выборочное среднее $\overline{X}_n$ и выборочная дисперсия
\[
    S_n ^ 2 = \frac1{n - 1} \sum_{j=1}^{n} \left(X_j - \overline{X}_n\right) ^ 2
\]
независимы, причем $\left(n - 1\right)\sigma ^ 2 S_n ^ 2$ имеет хи квадрат распределение с $n - 1$ степенью свободы $\chi^2_{n-1}$. Напомним, что
\[
    \rho_{\chi^2_{n}}\left(t\right) = C_n t ^ {\frac{n-2}2} e ^ {-\frac{t}2} \text{Ind}_{t > 0}
\]
Рассмотрим статистику
\[
    T_{n-1}\left(X\right) =\frac{\sqrt{n}\left(\overline{X}_n - \mu\right)}{\sqrt{S_n ^ 2}} = \frac{\sqrt{n}\frac{\left(\overline{X}_n - \mu\right)}{\sigma}}{\sqrt{\sigma ^{-2} S_n ^ 2}} = \frac{Z_n}{\sqrt{R_n}},
\]
где $Z_n \sim N\left(0, 1\right)$, $R_n \sim \frac 1 {n-1} \chi^2_{n-1}$ и $Z_n$ и $R_n$независимы. Распределение $T_{n-1}$ имеет плотность 
\[
    \rho\left(t\right) = C_n \left(1 + \frac{t^2}{n - 1}\right) ^ {-n/2}
\]
и называется распределением Стьюдента с $n - 1$ степенью свободы. Получаем, что доверительный интервал уровня доверия $1 - \alpha$ имеет вид
\[
    \left(\overline{X}_n - \frac{\sqrt{S^2_n} t_{1-\alpha/2}}{\sqrt n}, \overline{X}_n + \frac{\sqrt{S^2_n} t_{1-\alpha/2}}{\sqrt n}\right), \ F_{T_{n-1}}\left(t_{1-\alpha/2}\right) = 1-\alpha/2
\]
\par
\textbf{Доверительный интервал для $\sigma$} строится с помощью центральной статистики
\[
    \sigma ^ {-2}\left(n - 1\right)S_n ^ 2 \sim \chi_{n-1}^2
\]
Пусть $F_{\chi^2}\left(x_{\alpha/2}\right) = \alpha/2$ и $F_{\chi^2}\left(x_{1-\alpha/2}\right) = 1-\alpha/2$. Тогда 
\[
    P\left(x_{\alpha/2} < \frac{\left(n-1\right) S^2_n}{\sigma^2} < x_{1-\alpha/2}\right) = 1 - \alpha
\]
Доверительный интервал уровня доверия $1 - \alpha$ имеет вид
\[
    \left(\sqrt{\frac{\left(n-1\right) S^2_n}{x_{1-\alpha/2}}}, \sqrt{\frac{\left(n-1\right) S^2_n}{x_{\alpha/2}}}\right)
\]
\clearpage

\section{Проверка гипотез. Критерий Неймана-Пирсона}

Пусть есть выборка $X = \left(X_1, \cdots, X_n\right)$ из параметрического семейства с параметром $\theta$. Какое-либо предположение о параметре $\theta$ называется \textit{статистической гипотезой}. Статистическая гипотеза называется простой, если предположение заключается в том, что $\theta = \theta_0$ (то есть мы по выборке предлагаем конкретную оценку для $\theta$). Наша цель состоит в построении статистического критерия, который по выборке принимает или отклоняет гипотезу $H_0$ (проверка гипотез). Идея построения критерия заключается в том, что получение маловероятных по $H_0$ значений выборки свидетельствуют против нее (например, если все значения выборки получились маловероятными, то, скорее всего, с нашей гипотезой что-то не так). Обычно критерий строится с помощью \textit{критического множетсва} $K \ - $ такое множество в области значений выборки, что при выполнении гипотезы $H_0$ вероятность попадания в это множество мала. Если $X \in K$, то гипотеза $H_0$ отклоняется и принимается в ином случае.
\par
Обычно вместе с основной гипотезой $H_0$ задана еще и альтернативная гипотеза $H_1$. При таком подходе можно совершить ошибки двух видов:
\begin{enumerate}
    \item \textit{Ошибка первого рода}. Когда мы отклоняем истинную гипотезу $H_0$. Вероятность такой ошибки называется \textit{уровнем значимости критерия};
    \item \textit{Ошибка второго рода}. Когда мы принимаем ложную гипотезу $H_0$, то есть принимаем $H_0$ при истинности $H_1$;
\end{enumerate}
\par
В случае простых гипотез ($H_0: \theta = \theta_0$ и $H_1: \theta = \theta_1$) вероятность ошибки первого рода $\alpha = P_{\theta_0}\left(X \in K\right)$, а вероятность ошибки второго рода $\beta = P_{\theta_1}\left(X \not\in K\right)$. Величину $1 - \beta$ называют \textit{мощностью критерия} (хочется мощность иметь побольше).
\par 
Функцию $W\left(K, \theta\right) = P_\theta\left(X \in K\right)$ называют функцией мощности критерия $K$. Обычно задача ставится следющим образом. При фиксированном уровне значимости $\alpha$ (то есть $W\left(K, \theta\right) \leq \alpha$ для всех $\theta$ удовлетворяющих гипотезе $H_0$) найти критерий наибольшей мощности (то есть $W\left(K, \theta\right) \geq W\left(S, \theta\right)$ для всех $\theta$ удовлетворяющих гипотезе $H_1$, для каждого другого критерия $S$ уровня занчимости $\alpha$). То есть при фиксированном уровне ошибки первого рода минимизируем ошибку второго рода.

\begin{example}
Пусть $X = \left(X_1, \cdots, X_n\right) \ -$ выборка из $N\left(\mu, \sigma ^ 2\right)$. Предположим, мы хотим проверить гипотезу $H_0$, утверждающую, что $\mu = \mu_0$, против альтернативной гипотезы $H_1$: $\mu = \mu_1$. Ранее мы строили для $\mu$ доверительный интервал уровня $1 - \alpha$:
\[
    P_\mu\left(\overline{X}_n - \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}} \leq \mu \leq \overline{X}_n + \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}}\right)  = 1 - \alpha
\]
Построим критерий на основе доверительного интервала:
\[
    K = \left\{X: \overline{X}_n > \mu_0 + \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}}\right\} \cup \left\{X: \overline{X}_n < \mu_0 - \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}}\right\}
\]
Получаем уровень значимости $\alpha$. Мощность критерия равна $1 - \beta$, где $\beta \ - $ вероятность того, что $\mu_0$ принадлежит указанному доверительному интервалу при условии $\mu = \mu_1$. Вычислим $\beta$:
\[
    \beta = P_{\mu_1}\left(\overline{X}_n - \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}} \leq \mu_0 \leq \overline{X}_n + \frac{\sigma z_{1 - \alpha/2}}{\sqrt{n}}\right) =\]\[= P_{\mu_1}\left(\frac{\sqrt{n}\left(\mu_0 - \mu_1\right)}{\sigma} - z_{1 - \frac \alpha 2} \leq \frac{\sqrt{n}\left(\overline{X}_n - \mu_1\right)}{\sigma} \leq \frac{\sqrt{n}\left(\mu_0 - \mu_1\right)}{\sigma} + z_{1 - \frac \alpha 2}\right) = 
\]\[
    = \Phi\left(\frac{\sqrt{n}\left(\mu_0 - \mu_1\right)}{\sigma} + z_{1 - \frac \alpha 2}\right) - \Phi\left(\frac{\sqrt{n}\left(\mu_0 - \mu_1\right)}{\sigma} - z_{1 - \frac \alpha 2}\right)
\]
Заметим, что вероятность ошибки второго рода стремится к нулю (мощность критерия к единице) при $n \to \infty$.
\par 
Если с ростом объема выборки величина $1 - \beta$ стремится к единице, то критерий называется \textit{состоятельным}.
\end{example}
\par
Пусть $H_0$ состоит в том, что выборка $X = \left(X_1, \cdots, X_n\right)$ имеет плотность распределения $f_0$, а альтернативная гипотеза $H_1$ состоит в том, что выборка имеет плотность распределения $f_1$. Предположим, что $f_0$ и $f_1$ положительны на $\mathbb{R} ^ n$. Рассмотрим набор множеств 
\[
    K_t  = \left\{x \in \mathbb{R} ^ n: \frac{f_1\left(x\right)}{f_0\left(x\right)}\geq t\right\}, \ t\geq 0
\]
Заметим, что $P_0 \left(X \in K\right) = \int_K f_0$, а $P_1\left(X \in K\right) = \int_K f_1$, то есть мы строим такое критическое множество, чтобы ошибка первого рода была поменьше, а второго побольше. Предположим, что для всякого $\alpha \in \left(0, 1\right)$ существует $t\left(\alpha\right) \geq 0$ такое, что $P_0\left(X \in K_{t\left(\alpha\right)}\right) = \alpha$.
\begin{theorem} (Неймана-Пирсона) \\
Наиболее мощный критерий уровня значимости $\alpha$ задается критическим множеством $K = K_{t\left(\alpha\right)}$
\end{theorem}
\begin{proof}
Рассмотрим другой критерий $Q$ уровня значимости $P_0\left(X \in Q\right) \leq \alpha = P\left(X \in K\right)$. Сравним мощности этих критериев:
\[
    P_1\left(X \in K\right) - P_1\left(X \in Q\right) = \int_K f_1\left(x\right) dx - \int_Q f_1\left(x\right) dx = \int_{K \backslash Q} f_1\left(x\right) dx - \int_{Q \backslash K} f_1\left(x\right) dx \geq 
\]
Так как $t\left(\alpha\right) f_0\left(x\right) \leq f_1\left(x\right)$ для $x \in K_{t\left(\alpha\right)}$, и $t\left(\alpha\right) f_0\left(x\right) \geq f_1\left(x\right)$ для $x \not\in K_{t\left(\alpha\right)}$, то
\[
    \geq t\left(\alpha\right)\int_{K \backslash Q} f_0\left(x\right) dx - t\left(\alpha\right)\int_{Q \backslash K} f_0\left(x\right) dx = t\left(\alpha\right) \left(P_0\left(X \in K\right) - P_0\left(X \in Q\right)\right) \geq 0
\]
\end{proof}

\begin{example}
Пусть $X = \left(X_1, \cdots, X_n\right)$ выборка из $N\left(\theta, 1\right)$. Гипотеза $H_0$: $\theta = \theta_0$, альтернатива $H_1$: $\theta > \theta_0$. Построим по теореме Неймана-Пирсона наиболее мощный критерий при уровне значимости $\alpha$. Рассмотрим систему множеств (делим и умножаем стандартные плотности):
\[
    K_t = \left\{x: \ \exp\left(-\frac 1 2 \sum_{j = 1}^{n}\left(x_j - \theta\right) ^ 2 + \frac 1 2 \sum_{j = 1}^{n} \left(x_j - \theta_0\right) ^ 2\right) \geq t\right\}
\]
Данная система равносильна системе множеств (раскрываем квадраты и сокращаем):
\[
    \left\{x: \ \left(\theta - \theta_0\right) \sum_{j = 1}^{n} x_j \geq \tau \right\}
\]
Так как $\theta > \theta_0$, получаем равносильное семейство
\[
    \left\{x: \ \overline{x}_n \geq c \right\}
\]
Находим $c$:
\[
    \alpha = P_{\theta_0}\left(\overline{x}_n \geq c\right) = P_{\theta_0}\left(\frac{\sqrt{n} \left(\overline{x}_n - \theta_0\right)}{1} \geq \frac{\sqrt{n}\left(c - \theta_0\right)}{1}\right) = 1 - \Phi\left(\sqrt{n}\left(c - \theta_0\right)\right)
\]
То есть берем $c = \theta_0 + \frac{z_{1-\alpha}}{\sqrt{n}}$. Вычисляем мощность:
\[
    1 - \beta = P_\theta\left(\overline{x}_n \geq c \right) = 1 - \Phi\left(\sqrt{n}\left(c - \theta\right)\right) = 1 - \Phi\left(z_{1-\alpha} + \sqrt{n}\left(\theta_0 - \theta\right)\right) \xrightarrow{n \to \infty} 1
\]
То есть мощность состоятельная.
\end{example}
При построении критерия, мы стремимся минимизировать ошибки первого и второго рода. Оценим снизу сумму вероятностей ошибок в случае простых гипотез (выбираем $f_0$ или $f_1$):
\[
    \alpha + \beta = P_0\left(X \in K\right) + P_1\left(X \not\in K\right) = \int_K f_0\left(x\right) dx + \int_{\mathbb{R} ^ n \backslash K} f_1\left(x\right) dx = 1 + \int_K \left(f_0\left(x\right) - f_1\left(x\right)\right)dx
\]
Если теперь $S = \left\{x: \ f_0\left(x\right) < f_1\left(x\right)\right\}$, то (если все с минусом попереносить, то получим $1 = 1$)
\[
    \int_S \left(f_0\left(x\right) - f_1\left(x\right)\right) dx = \int_{\mathbb{R} ^ n \backslash S } \left(f_1\left(x\right) - f_0\left(x\right)\right)dx = -\frac 1 2 \int_{\mathbb{R} ^ n} \abs{f_0\left(x\right) - f_1\left(x\right)} dx
\]
Значит, (оставляем все, что идет с минусом)
\[
    \alpha + \beta \geq 1 + \int_{K \cap S} \left(f_0\left(x\right) - f_1\left(x\right)\right) dx \geq 1 + \int_{S} \left(f_0\left(x\right) - f_1\left(x\right)\right) dx = 1 - \frac 1 2 \int_{\mathbb{R} ^ n} \abs{f_0\left(x\right) - f_1\left(x\right)} dx
\]
\par 
Таким образом, если плотности похожи друг на друга, то сумма вероятностей ошибки примерно равна 1, то есть не получится одновременно уменьшить вероятности обеих ошибок. Если же плотности сильно различаются, то сумма вероятностей примерно равна нулю, что естественно. Обычно добиться сильного различия плотностей можно лишь увеличением объема выборки.

\clearpage

\section{$\chi ^ 2$-критерий Пирсона}
Пусть $X = \left(X_1, \cdots, X_n\right) \ - $ выборка из дискретного распределения, при котором случайная величина принимает значения $a_1, \cdots, a_k$ с вероятностями $p_1, \cdots, p_k$. Пусть основная гипотеза $H_0$ заключается в том, что $p_i = p_{i_0}$ $\forall i \in \left\{1, \cdots, k\right\}$. А альтернативная гипотеза $H_1$ заключается в том, что распределения вероятностей отлично от $\left\{p_{1_0}, \cdots, p_{k_0}\right\}$. Пусть $\nu_i \ - $ количество $a_i$ в $X$.


\begin{theorem}
При выполнении гипотезы $H_0$ случайная величина $\overline{\chi} ^ 2 = \sum_{i= 1}^{k}\left(\frac{\nu_i - np_{i_0}}{\sqrt{n p_{i_0}}}\right) ^ 2$ сходится по распределению к $\chi ^ 2_{k - 1}$.
\end{theorem}
\begin{proof}

Заметим, что $\nu_i = \sum_{j = 1}^{n} \text{Ind}_{X_j = a_i}$. Рассмотрим случайный вектор \\
$
    \left(\frac{\nu_1 - n p_{1_0}}{\sqrt{n p_{1_0}}}, \cdots, \frac{\nu_k - n p_{k_0}}{\sqrt{n p_{k_0}}}\right).
$
По многомерной ЦПТ (теорема 8) данный вектор сходится по распределению к многомерному нормальному вектору $Z = \left(Z_1, \cdots, Z_k\right)$ с нулевым вектором средних и матрицей ковариации $R = \left(r_{ij}\right)$, где
\[
    r_{ii} = \text{cov}\left(\frac{\text{Ind}_{X_1 = a_i}}{\sqrt{p_{i_0}}}, \frac{\text{Ind}_{X_1 = a_i}}{\sqrt{p_{i_0}}}\right) = \mathbb{D} \frac{\text{Ind}_{X_1 = a_i}}{\sqrt{p_{i_0}}} = \frac{p_{i_0} \left(1 - p_{i_0}\right)}{p_{i_0}} = 1 - p_{i_0}
\]\[
    r_{ij} = \text{cov}\left(\frac{\text{Ind}_{X_1 = a_i}}{\sqrt{p_{i_0}}}, \frac{\text{Ind}_{X_1 = a_j}}{\sqrt{p_{j_0}}}\right) = \frac{1}{\sqrt{p_{i_0}p_{j_0}}} \left(\mathbb{E}\left[\text{Ind}_{X_1 = a_i}\text{Ind}_{X_1 = a_j}\right] - p_{i_0}p_{j_0}\right) = \frac{1}{\sqrt{p_{i_0}p_{j_0}}} \left(0 - p_{i_0}p_{j_0}\right) = -\sqrt{p_{i_0}p_{j_0}}
\]
По теореме о наследовании сходимости (предложение 5) случайные величины $\overline{\chi} ^ 2$ сходятся по распределению к $\abs{Z} ^ 2 = Z_1 ^ 2 + \cdots + Z_k ^ 2$. Покажем, что $\abs{Z} ^ 2 \sim \chi ^ 2_{k - 1}$. Заметим, что верно равенство
\[
    R = E - \left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right) ^ T \left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right)
\]
Рассмотрим ортогональную матрицу, у которой первая строчка имеет вид $ \left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right)$. Тогда ковариационная матрица случайного вектора $V = CZ$ имеет вид:
\[
    CRC ^ T = E - C\left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right) ^ T \left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right) C ^ T = E - e_{11}
\]
 где $e_{11} \ -$ матрица, где $1$ стоит в первой ячейки, а все остальное нули (так получается, потому что в $\left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right) C ^ T$ при умножении на первый столбец мы получаем 1, так как просуммировали вероятности, а дальше мы вектор умножаем на столбцы, ортогональные ему, получаем $e_{11}$, аналогично в $C\left(\sqrt{p_{1_0}}, \cdots, \sqrt{p_{k_0}}\right) ^ T$). Следовательно $v_1 \equiv 0$ (так как нулевая дисперсия). Так как мы умножили на ортогональную матрицу, то длина вектора сохранилась, отсюда по определению $\chi ^ 2_{k - 1}$ получаем
 \[
    \abs{Z} ^ 2 = \abs{V} ^ 2 = v_2 ^ 2 + \cdots + v_k ^ 2 \sim \chi ^2_{k - 1}
 \]
\end{proof}
\par 
Отсюда можем сформулировать следующий критерий уровня значимости $\alpha$: пусть $x_\alpha$ выбрано так, что $P\left(\chi ^ 2_{k - 1}\geq x_{\alpha}\right) = \alpha$. Тогда по доказанному, в случае справедливости гипотезы $H_0$
\[
    P\left(\sum_{i= 1}^{k}\left(\frac{\nu_i - np_{i_0}}{\sqrt{n p_{i_0}}}\right) ^ 2 \geq x_{\alpha}\right) \xrightarrow{n \to \infty} \alpha
\]
Если на конкретной реализации выборки мы получаем 
\[
    \sum_{i= 1}^{k}\left(\frac{\nu_i - np_{i_0}}{\sqrt{n p_{i_0}}}\right) ^ 2 < x_{\alpha},
\]
то $H_0$ принимается, иначе отклоняется.

\clearpage

\section{Эмпирическая функция распределения и теорема Гливенко-Кантелли}
Пусть $X = \left(X_1, \cdots, X_n\right) \ -$ выборка из распределения с функцией распределения $F$. Рассмотрим на $\mathbb{R}$ равномерное вероятностное распределение, расположенное в точка $X_1, \cdots, X_n$, то есть $P^*\left(B\right) = \frac{\#\left\{X_j \in B\right\}}{n}$. Построена последовательность эмпирических распределений. Для эмпирических распределений справедлива следующая теорема.

\begin{theorem}
Для произвольного множества $B \in \mathcal{B}\left(\mathbb{R}\right)$, $P_n^*\left(B\right) \xrightarrow{\text{п.н.}} P\left(X_1 \in B\right)$ (то есть эмпирическая вероятность стремится к реальной)
\end{theorem}
\begin{proof}
Заметим, что  $P^*\left(B\right) = \frac 1 n \sum_{j=1}^n \text{Ind}_{X_j \in B}$. По УЗБЧ: $P^*\left(B\right) \xrightarrow{} \mathbb{E} \text{Ind}_{X_1 \in B} = P\left(X_1 \in B\right)$
\end{proof}
\begin{definition}
Случайную величину 
\[
    F^*_n\left(t\right)= \frac 1 n \sum_{j=1}^n \text{Ind}_{X_j \in \left(-\infty, t\right]}
\]
называют \textit{эмпирической функцией распределения}. Так как
\[
    \mathbb{E} F^*_n\left(t\right) = \frac 1 n \sum_{j=1}^n P\left(X_j \leq t\right) = P\left(X_1 \leq t\right) = F\left(t\right),
\]
то $F_n^*$ является несмещенной оценкой функции распределения $F$.
\end{definition}

\begin{theorem} (Теорема Гливенко-Кантелли) \\
Величина $\sup_{t \in \mathbb{R}} \abs{F_n\left(t\right) - F\left(t\right)} \xrightarrow{\text{п.н.}} 0$ (то есть имеет место равномерная сходимость)

\end{theorem}
\begin{proof}
Доказательство проведем только для непрерывной $F$. Зафиксируем $N \in \mathbb{N}$. Пусть точки $t_0, \cdots, t_N$ выбраны так, что $F\left(t_0\right) = 0$, $F\left(t_1\right) = \frac 1 {N}$, $\cdots$, $F\left(t_N\right) = 1$, в частности $t_0 = -\infty$, $t_N = +\infty$. Тогда при $t \in \left[t_k, t_{k + 1}\right)$ выполнено
\[
    F^*_n\left(t\right) - F\left(t\right)\leq F^*_n\left(t_{k + 1}\right) - F\left(t_k\right) = F^*_n\left(t_{k + 1}\right) - F\left(t_{k + 1}\right) + \frac 1 N
\]\[
    F^*_n\left(t\right) - F\left(t\right) \geq F^*_n\left(t_k\right) - F\left(t_{k + 1}\right) = F^*_n\left(t_k\right) - F\left(t_k\right) - \frac 1 N
\]
То есть знаем, что
\[
    \abs{F_n^*\left(t\right) - F\left(t\right)} \leq \sup_{k} \abs{F_n^*\left(t_k\right) - F\left(t_k\right)} + \frac 1 N
\]
\par
Теперь заметим, что для каждой точки $t_k$ по УЗБЧ (аналогично предыдущей теореме) $F^*_n\left(t_k\right) \xrightarrow{\text{п.н.}} F\left(t_k\right)$. Введем событие $A_k = \left\{F^*_n\left(t_k\right) \xrightarrow{\text{п.н.}} F\left(t_k\right)\right\}$, ясно, что $P\left(A_k\right) = 1$ для $k \in \left\{1, \cdots, N - 1\right\}$. Рассмотрим событие $A = \bigcap_{k=1}^{N-1} A_k$, тогда справедливо равенство $\overline{A} = \bigcup_{k=1}^{N - 1} \overline{A}_k$. Очевидно, что \\  $P\left(\overline{A}\right) \leq \sum_{k=1}^{N - 1} P\left(\overline{A}_k\right) = 0$, следовательно $P\left(\overline{A}\right) = 0$, а $P\left(A\right) = 1$. Грубо говоря, на множестве исходов из $A$ (которое по сути почти все множество исходов) имеем (для любого $\varepsilon > 0$, начиная с некоторого $N_0$)
\[
    \sup_{t \in \mathbb{R}} \abs{F_n^*\left(t\right) - F\left(t\right)} \leq \sup_{k} \abs{F_n^*\left(t_k\right) - F\left(t_k\right)} + \frac 1 N \leq \varepsilon + \frac 1 N
\]
Теперь $N \to \infty$ и получаем 
\[
    \lim_{n \to \infty} \sup_{t \in \mathbb{R}} \abs{F_n^*\left(t\right) - F\left(t\right)} = 0
\] 
с вероятностью $P\left(A\right) = 1$, то есть почти наверное.
\end{proof}

\par 
Обычно для построения эмпирической функции распределения используют следующую процедуру. Элементы выборки упорядочивают по возрастанию: $X_{\left(1\right)}, \cdots, X_{\left(n\right)}$. Тогда $F^*_n\left(t\right) = \frac k n$ при $t \in \left[X_{\left(k\right)}, X_{\left(k + 1\right)}\right)$
\end{document}
